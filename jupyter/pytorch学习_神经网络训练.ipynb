{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# 向量类\n",
    "class Vector:\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(\"+str(self.x)+\",\"+str(self.y)+\")\"\n",
    "\n",
    "\t# 判断象限的方法\n",
    "    @staticmethod\n",
    "    def judge_quadrant(vec):\n",
    "        if not isinstance(vec, Vector):\n",
    "            raise Exception\n",
    "\n",
    "        if vec.x > 0 and vec.y > 0:\n",
    "            return 0\n",
    "        elif vec.x < 0 and vec.y > 0:\n",
    "            return 1\n",
    "        elif vec.x < 0 and vec.y < 0:\n",
    "            return 2\n",
    "        elif vec.x > 0 and vec.y < 0:\n",
    "            return 3\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        # Toy dataset 准备数据集\n",
    "import random\n",
    "\n",
    "data_size = 12800\n",
    "dataset = []\n",
    "for i in range(data_size):\n",
    "    x = random.randint(-1000, 1000)\n",
    "    y = random.randint(-1000, 1000)\n",
    "    vec = Vector(x, y)\n",
    "    dataset.append(vec)\n",
    "\t\n",
    "x_train = torch.Tensor([[vec.x, vec.y] for vec in dataset]) # 使用pytorch张量的格式\n",
    "label_set = [Vector.judge_quadrant(vec) for vec in dataset]\n",
    "y_train = []\n",
    "for label in label_set:\n",
    "    if label == 0:\n",
    "        y_train.append([1, 0, 0, 0])\n",
    "    elif label == 1:\n",
    "        y_train.append([0, 1, 0, 0])\n",
    "    elif label == 2:\n",
    "        y_train.append([0, 0, 1, 0])\n",
    "    elif label == 3:\n",
    "        y_train.append([0, 0, 0, 1])\n",
    "\n",
    "y_train = torch.Tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset 准备数据集\n",
    "import random\n",
    "\n",
    "data_size = 12800\n",
    "dataset = []\n",
    "for i in range(data_size):\n",
    "    x = random.randint(-1000, 1000)\n",
    "    y = random.randint(-1000, 1000)\n",
    "    vec = Vector(x, y)\n",
    "    dataset.append(vec)\n",
    "\t\n",
    "x_train = torch.Tensor([[vec.x, vec.y] for vec in dataset]) # 使用pytorch张量的格式\n",
    "label_set = [Vector.judge_quadrant(vec) for vec in dataset]\n",
    "y_train = []\n",
    "for label in label_set:\n",
    "    if label == 0:\n",
    "        y_train.append([1, 0, 0, 0])\n",
    "    elif label == 1:\n",
    "        y_train.append([0, 1, 0, 0])\n",
    "    elif label == 2:\n",
    "        y_train.append([0, 0, 1, 0])\n",
    "    elif label == 3:\n",
    "        y_train.append([0, 0, 0, 1])\n",
    "\n",
    "y_train = torch.Tensor(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ToyNet(nn.Module):\n",
    "\n",
    "\t# 这里定义网络层的信息\n",
    "    def __init__(self):\n",
    "        super(ToyNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.fc2 = nn.Linear(16, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "\t# 这里构建前向传播计算过程\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 times loss: tensor(2489.0427, grad_fn=<MseLossBackward0>)\n",
      "1 times loss: tensor(2050.3796, grad_fn=<MseLossBackward0>)\n",
      "2 times loss: tensor(1717.7178, grad_fn=<MseLossBackward0>)\n",
      "3 times loss: tensor(1460.1031, grad_fn=<MseLossBackward0>)\n",
      "4 times loss: tensor(1257.2673, grad_fn=<MseLossBackward0>)\n",
      "5 times loss: tensor(1095.3048, grad_fn=<MseLossBackward0>)\n",
      "6 times loss: tensor(964.0306, grad_fn=<MseLossBackward0>)\n",
      "7 times loss: tensor(856.1373, grad_fn=<MseLossBackward0>)\n",
      "8 times loss: tensor(766.3474, grad_fn=<MseLossBackward0>)\n",
      "9 times loss: tensor(690.7985, grad_fn=<MseLossBackward0>)\n",
      "10 times loss: tensor(626.6115, grad_fn=<MseLossBackward0>)\n",
      "11 times loss: tensor(571.5990, grad_fn=<MseLossBackward0>)\n",
      "12 times loss: tensor(524.0851, grad_fn=<MseLossBackward0>)\n",
      "13 times loss: tensor(482.7712, grad_fn=<MseLossBackward0>)\n",
      "14 times loss: tensor(446.6289, grad_fn=<MseLossBackward0>)\n",
      "15 times loss: tensor(414.8440, grad_fn=<MseLossBackward0>)\n",
      "16 times loss: tensor(386.7560, grad_fn=<MseLossBackward0>)\n",
      "17 times loss: tensor(361.8307, grad_fn=<MseLossBackward0>)\n",
      "18 times loss: tensor(339.6237, grad_fn=<MseLossBackward0>)\n",
      "19 times loss: tensor(319.7653, grad_fn=<MseLossBackward0>)\n",
      "20 times loss: tensor(301.9470, grad_fn=<MseLossBackward0>)\n",
      "21 times loss: tensor(285.9088, grad_fn=<MseLossBackward0>)\n",
      "22 times loss: tensor(271.4292, grad_fn=<MseLossBackward0>)\n",
      "23 times loss: tensor(258.3176, grad_fn=<MseLossBackward0>)\n",
      "24 times loss: tensor(246.4106, grad_fn=<MseLossBackward0>)\n",
      "25 times loss: tensor(235.5668, grad_fn=<MseLossBackward0>)\n",
      "26 times loss: tensor(225.6632, grad_fn=<MseLossBackward0>)\n",
      "27 times loss: tensor(216.5936, grad_fn=<MseLossBackward0>)\n",
      "28 times loss: tensor(208.2651, grad_fn=<MseLossBackward0>)\n",
      "29 times loss: tensor(200.5963, grad_fn=<MseLossBackward0>)\n",
      "30 times loss: tensor(193.5168, grad_fn=<MseLossBackward0>)\n",
      "31 times loss: tensor(186.9643, grad_fn=<MseLossBackward0>)\n",
      "32 times loss: tensor(180.8839, grad_fn=<MseLossBackward0>)\n",
      "33 times loss: tensor(175.2259, grad_fn=<MseLossBackward0>)\n",
      "34 times loss: tensor(169.9479, grad_fn=<MseLossBackward0>)\n",
      "35 times loss: tensor(165.0116, grad_fn=<MseLossBackward0>)\n",
      "36 times loss: tensor(160.3830, grad_fn=<MseLossBackward0>)\n",
      "37 times loss: tensor(156.0326, grad_fn=<MseLossBackward0>)\n",
      "38 times loss: tensor(151.9341, grad_fn=<MseLossBackward0>)\n",
      "39 times loss: tensor(148.0644, grad_fn=<MseLossBackward0>)\n",
      "40 times loss: tensor(144.4023, grad_fn=<MseLossBackward0>)\n",
      "41 times loss: tensor(140.9291, grad_fn=<MseLossBackward0>)\n",
      "42 times loss: tensor(137.6283, grad_fn=<MseLossBackward0>)\n",
      "43 times loss: tensor(134.4857, grad_fn=<MseLossBackward0>)\n",
      "44 times loss: tensor(131.4882, grad_fn=<MseLossBackward0>)\n",
      "45 times loss: tensor(128.6242, grad_fn=<MseLossBackward0>)\n",
      "46 times loss: tensor(125.8829, grad_fn=<MseLossBackward0>)\n",
      "47 times loss: tensor(123.2547, grad_fn=<MseLossBackward0>)\n",
      "48 times loss: tensor(120.7315, grad_fn=<MseLossBackward0>)\n",
      "49 times loss: tensor(118.3060, grad_fn=<MseLossBackward0>)\n",
      "50 times loss: tensor(115.9712, grad_fn=<MseLossBackward0>)\n",
      "51 times loss: tensor(113.7207, grad_fn=<MseLossBackward0>)\n",
      "52 times loss: tensor(111.5491, grad_fn=<MseLossBackward0>)\n",
      "53 times loss: tensor(109.4516, grad_fn=<MseLossBackward0>)\n",
      "54 times loss: tensor(107.4235, grad_fn=<MseLossBackward0>)\n",
      "55 times loss: tensor(105.4605, grad_fn=<MseLossBackward0>)\n",
      "56 times loss: tensor(103.5591, grad_fn=<MseLossBackward0>)\n",
      "57 times loss: tensor(101.7158, grad_fn=<MseLossBackward0>)\n",
      "58 times loss: tensor(99.9274, grad_fn=<MseLossBackward0>)\n",
      "59 times loss: tensor(98.1909, grad_fn=<MseLossBackward0>)\n",
      "60 times loss: tensor(96.5040, grad_fn=<MseLossBackward0>)\n",
      "61 times loss: tensor(94.8642, grad_fn=<MseLossBackward0>)\n",
      "62 times loss: tensor(93.2693, grad_fn=<MseLossBackward0>)\n",
      "63 times loss: tensor(91.7172, grad_fn=<MseLossBackward0>)\n",
      "64 times loss: tensor(90.2061, grad_fn=<MseLossBackward0>)\n",
      "65 times loss: tensor(88.7343, grad_fn=<MseLossBackward0>)\n",
      "66 times loss: tensor(87.3002, grad_fn=<MseLossBackward0>)\n",
      "67 times loss: tensor(85.9022, grad_fn=<MseLossBackward0>)\n",
      "68 times loss: tensor(84.5389, grad_fn=<MseLossBackward0>)\n",
      "69 times loss: tensor(83.2091, grad_fn=<MseLossBackward0>)\n",
      "70 times loss: tensor(81.9115, grad_fn=<MseLossBackward0>)\n",
      "71 times loss: tensor(80.6450, grad_fn=<MseLossBackward0>)\n",
      "72 times loss: tensor(79.4084, grad_fn=<MseLossBackward0>)\n",
      "73 times loss: tensor(78.2006, grad_fn=<MseLossBackward0>)\n",
      "74 times loss: tensor(77.0210, grad_fn=<MseLossBackward0>)\n",
      "75 times loss: tensor(75.8685, grad_fn=<MseLossBackward0>)\n",
      "76 times loss: tensor(74.7423, grad_fn=<MseLossBackward0>)\n",
      "77 times loss: tensor(73.6415, grad_fn=<MseLossBackward0>)\n",
      "78 times loss: tensor(72.5654, grad_fn=<MseLossBackward0>)\n",
      "79 times loss: tensor(71.5131, grad_fn=<MseLossBackward0>)\n",
      "80 times loss: tensor(70.4839, grad_fn=<MseLossBackward0>)\n",
      "81 times loss: tensor(69.4773, grad_fn=<MseLossBackward0>)\n",
      "82 times loss: tensor(68.4925, grad_fn=<MseLossBackward0>)\n",
      "83 times loss: tensor(67.5289, grad_fn=<MseLossBackward0>)\n",
      "84 times loss: tensor(66.5860, grad_fn=<MseLossBackward0>)\n",
      "85 times loss: tensor(65.6631, grad_fn=<MseLossBackward0>)\n",
      "86 times loss: tensor(64.7597, grad_fn=<MseLossBackward0>)\n",
      "87 times loss: tensor(63.8753, grad_fn=<MseLossBackward0>)\n",
      "88 times loss: tensor(63.0095, grad_fn=<MseLossBackward0>)\n",
      "89 times loss: tensor(62.1617, grad_fn=<MseLossBackward0>)\n",
      "90 times loss: tensor(61.3315, grad_fn=<MseLossBackward0>)\n",
      "91 times loss: tensor(60.5184, grad_fn=<MseLossBackward0>)\n",
      "92 times loss: tensor(59.7221, grad_fn=<MseLossBackward0>)\n",
      "93 times loss: tensor(58.9419, grad_fn=<MseLossBackward0>)\n",
      "94 times loss: tensor(58.1777, grad_fn=<MseLossBackward0>)\n",
      "95 times loss: tensor(57.4291, grad_fn=<MseLossBackward0>)\n",
      "96 times loss: tensor(56.6956, grad_fn=<MseLossBackward0>)\n",
      "97 times loss: tensor(55.9769, grad_fn=<MseLossBackward0>)\n",
      "98 times loss: tensor(55.2726, grad_fn=<MseLossBackward0>)\n",
      "99 times loss: tensor(54.5822, grad_fn=<MseLossBackward0>)\n",
      "100 times loss: tensor(53.9057, grad_fn=<MseLossBackward0>)\n",
      "101 times loss: tensor(53.2425, grad_fn=<MseLossBackward0>)\n",
      "102 times loss: tensor(52.5924, grad_fn=<MseLossBackward0>)\n",
      "103 times loss: tensor(51.9550, grad_fn=<MseLossBackward0>)\n",
      "104 times loss: tensor(51.3302, grad_fn=<MseLossBackward0>)\n",
      "105 times loss: tensor(50.7174, grad_fn=<MseLossBackward0>)\n",
      "106 times loss: tensor(50.1165, grad_fn=<MseLossBackward0>)\n",
      "107 times loss: tensor(49.5271, grad_fn=<MseLossBackward0>)\n",
      "108 times loss: tensor(48.9490, grad_fn=<MseLossBackward0>)\n",
      "109 times loss: tensor(48.3820, grad_fn=<MseLossBackward0>)\n",
      "110 times loss: tensor(47.8259, grad_fn=<MseLossBackward0>)\n",
      "111 times loss: tensor(47.2803, grad_fn=<MseLossBackward0>)\n",
      "112 times loss: tensor(46.7451, grad_fn=<MseLossBackward0>)\n",
      "113 times loss: tensor(46.2200, grad_fn=<MseLossBackward0>)\n",
      "114 times loss: tensor(45.7048, grad_fn=<MseLossBackward0>)\n",
      "115 times loss: tensor(45.1993, grad_fn=<MseLossBackward0>)\n",
      "116 times loss: tensor(44.7032, grad_fn=<MseLossBackward0>)\n",
      "117 times loss: tensor(44.2164, grad_fn=<MseLossBackward0>)\n",
      "118 times loss: tensor(43.7386, grad_fn=<MseLossBackward0>)\n",
      "119 times loss: tensor(43.2695, grad_fn=<MseLossBackward0>)\n",
      "120 times loss: tensor(42.8091, grad_fn=<MseLossBackward0>)\n",
      "121 times loss: tensor(42.3570, grad_fn=<MseLossBackward0>)\n",
      "122 times loss: tensor(41.9132, grad_fn=<MseLossBackward0>)\n",
      "123 times loss: tensor(41.4775, grad_fn=<MseLossBackward0>)\n",
      "124 times loss: tensor(41.0498, grad_fn=<MseLossBackward0>)\n",
      "125 times loss: tensor(40.6297, grad_fn=<MseLossBackward0>)\n",
      "126 times loss: tensor(40.2173, grad_fn=<MseLossBackward0>)\n",
      "127 times loss: tensor(39.8122, grad_fn=<MseLossBackward0>)\n",
      "128 times loss: tensor(39.4144, grad_fn=<MseLossBackward0>)\n",
      "129 times loss: tensor(39.0236, grad_fn=<MseLossBackward0>)\n",
      "130 times loss: tensor(38.6398, grad_fn=<MseLossBackward0>)\n",
      "131 times loss: tensor(38.2628, grad_fn=<MseLossBackward0>)\n",
      "132 times loss: tensor(37.8923, grad_fn=<MseLossBackward0>)\n",
      "133 times loss: tensor(37.5284, grad_fn=<MseLossBackward0>)\n",
      "134 times loss: tensor(37.1708, grad_fn=<MseLossBackward0>)\n",
      "135 times loss: tensor(36.8195, grad_fn=<MseLossBackward0>)\n",
      "136 times loss: tensor(36.4742, grad_fn=<MseLossBackward0>)\n",
      "137 times loss: tensor(36.1349, grad_fn=<MseLossBackward0>)\n",
      "138 times loss: tensor(35.8014, grad_fn=<MseLossBackward0>)\n",
      "139 times loss: tensor(35.4737, grad_fn=<MseLossBackward0>)\n",
      "140 times loss: tensor(35.1516, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 times loss: tensor(34.8350, grad_fn=<MseLossBackward0>)\n",
      "142 times loss: tensor(34.5237, grad_fn=<MseLossBackward0>)\n",
      "143 times loss: tensor(34.2176, grad_fn=<MseLossBackward0>)\n",
      "144 times loss: tensor(33.9167, grad_fn=<MseLossBackward0>)\n",
      "145 times loss: tensor(33.6208, grad_fn=<MseLossBackward0>)\n",
      "146 times loss: tensor(33.3299, grad_fn=<MseLossBackward0>)\n",
      "147 times loss: tensor(33.0438, grad_fn=<MseLossBackward0>)\n",
      "148 times loss: tensor(32.7623, grad_fn=<MseLossBackward0>)\n",
      "149 times loss: tensor(32.4855, grad_fn=<MseLossBackward0>)\n",
      "150 times loss: tensor(32.2133, grad_fn=<MseLossBackward0>)\n",
      "151 times loss: tensor(31.9454, grad_fn=<MseLossBackward0>)\n",
      "152 times loss: tensor(31.6819, grad_fn=<MseLossBackward0>)\n",
      "153 times loss: tensor(31.4226, grad_fn=<MseLossBackward0>)\n",
      "154 times loss: tensor(31.1676, grad_fn=<MseLossBackward0>)\n",
      "155 times loss: tensor(30.9166, grad_fn=<MseLossBackward0>)\n",
      "156 times loss: tensor(30.6697, grad_fn=<MseLossBackward0>)\n",
      "157 times loss: tensor(30.4267, grad_fn=<MseLossBackward0>)\n",
      "158 times loss: tensor(30.1876, grad_fn=<MseLossBackward0>)\n",
      "159 times loss: tensor(29.9523, grad_fn=<MseLossBackward0>)\n",
      "160 times loss: tensor(29.7206, grad_fn=<MseLossBackward0>)\n",
      "161 times loss: tensor(29.4926, grad_fn=<MseLossBackward0>)\n",
      "162 times loss: tensor(29.2681, grad_fn=<MseLossBackward0>)\n",
      "163 times loss: tensor(29.0471, grad_fn=<MseLossBackward0>)\n",
      "164 times loss: tensor(28.8296, grad_fn=<MseLossBackward0>)\n",
      "165 times loss: tensor(28.6153, grad_fn=<MseLossBackward0>)\n",
      "166 times loss: tensor(28.4044, grad_fn=<MseLossBackward0>)\n",
      "167 times loss: tensor(28.1967, grad_fn=<MseLossBackward0>)\n",
      "168 times loss: tensor(27.9921, grad_fn=<MseLossBackward0>)\n",
      "169 times loss: tensor(27.7906, grad_fn=<MseLossBackward0>)\n",
      "170 times loss: tensor(27.5922, grad_fn=<MseLossBackward0>)\n",
      "171 times loss: tensor(27.3967, grad_fn=<MseLossBackward0>)\n",
      "172 times loss: tensor(27.2042, grad_fn=<MseLossBackward0>)\n",
      "173 times loss: tensor(27.0145, grad_fn=<MseLossBackward0>)\n",
      "174 times loss: tensor(26.8277, grad_fn=<MseLossBackward0>)\n",
      "175 times loss: tensor(26.6436, grad_fn=<MseLossBackward0>)\n",
      "176 times loss: tensor(26.4622, grad_fn=<MseLossBackward0>)\n",
      "177 times loss: tensor(26.2835, grad_fn=<MseLossBackward0>)\n",
      "178 times loss: tensor(26.1074, grad_fn=<MseLossBackward0>)\n",
      "179 times loss: tensor(25.9338, grad_fn=<MseLossBackward0>)\n",
      "180 times loss: tensor(25.7628, grad_fn=<MseLossBackward0>)\n",
      "181 times loss: tensor(25.5942, grad_fn=<MseLossBackward0>)\n",
      "182 times loss: tensor(25.4279, grad_fn=<MseLossBackward0>)\n",
      "183 times loss: tensor(25.2640, grad_fn=<MseLossBackward0>)\n",
      "184 times loss: tensor(25.1024, grad_fn=<MseLossBackward0>)\n",
      "185 times loss: tensor(24.9432, grad_fn=<MseLossBackward0>)\n",
      "186 times loss: tensor(24.7861, grad_fn=<MseLossBackward0>)\n",
      "187 times loss: tensor(24.6313, grad_fn=<MseLossBackward0>)\n",
      "188 times loss: tensor(24.4786, grad_fn=<MseLossBackward0>)\n",
      "189 times loss: tensor(24.3280, grad_fn=<MseLossBackward0>)\n",
      "190 times loss: tensor(24.1795, grad_fn=<MseLossBackward0>)\n",
      "191 times loss: tensor(24.0330, grad_fn=<MseLossBackward0>)\n",
      "192 times loss: tensor(23.8885, grad_fn=<MseLossBackward0>)\n",
      "193 times loss: tensor(23.7459, grad_fn=<MseLossBackward0>)\n",
      "194 times loss: tensor(23.6053, grad_fn=<MseLossBackward0>)\n",
      "195 times loss: tensor(23.4666, grad_fn=<MseLossBackward0>)\n",
      "196 times loss: tensor(23.3298, grad_fn=<MseLossBackward0>)\n",
      "197 times loss: tensor(23.1948, grad_fn=<MseLossBackward0>)\n",
      "198 times loss: tensor(23.0615, grad_fn=<MseLossBackward0>)\n",
      "199 times loss: tensor(22.9301, grad_fn=<MseLossBackward0>)\n",
      "200 times loss: tensor(22.8003, grad_fn=<MseLossBackward0>)\n",
      "201 times loss: tensor(22.6723, grad_fn=<MseLossBackward0>)\n",
      "202 times loss: tensor(22.5458, grad_fn=<MseLossBackward0>)\n",
      "203 times loss: tensor(22.4211, grad_fn=<MseLossBackward0>)\n",
      "204 times loss: tensor(22.2979, grad_fn=<MseLossBackward0>)\n",
      "205 times loss: tensor(22.1763, grad_fn=<MseLossBackward0>)\n",
      "206 times loss: tensor(22.0562, grad_fn=<MseLossBackward0>)\n",
      "207 times loss: tensor(21.9377, grad_fn=<MseLossBackward0>)\n",
      "208 times loss: tensor(21.8207, grad_fn=<MseLossBackward0>)\n",
      "209 times loss: tensor(21.7052, grad_fn=<MseLossBackward0>)\n",
      "210 times loss: tensor(21.5911, grad_fn=<MseLossBackward0>)\n",
      "211 times loss: tensor(21.4784, grad_fn=<MseLossBackward0>)\n",
      "212 times loss: tensor(21.3671, grad_fn=<MseLossBackward0>)\n",
      "213 times loss: tensor(21.2572, grad_fn=<MseLossBackward0>)\n",
      "214 times loss: tensor(21.1486, grad_fn=<MseLossBackward0>)\n",
      "215 times loss: tensor(21.0413, grad_fn=<MseLossBackward0>)\n",
      "216 times loss: tensor(20.9354, grad_fn=<MseLossBackward0>)\n",
      "217 times loss: tensor(20.8307, grad_fn=<MseLossBackward0>)\n",
      "218 times loss: tensor(20.7272, grad_fn=<MseLossBackward0>)\n",
      "219 times loss: tensor(20.6251, grad_fn=<MseLossBackward0>)\n",
      "220 times loss: tensor(20.5241, grad_fn=<MseLossBackward0>)\n",
      "221 times loss: tensor(20.4243, grad_fn=<MseLossBackward0>)\n",
      "222 times loss: tensor(20.3256, grad_fn=<MseLossBackward0>)\n",
      "223 times loss: tensor(20.2282, grad_fn=<MseLossBackward0>)\n",
      "224 times loss: tensor(20.1318, grad_fn=<MseLossBackward0>)\n",
      "225 times loss: tensor(20.0365, grad_fn=<MseLossBackward0>)\n",
      "226 times loss: tensor(19.9424, grad_fn=<MseLossBackward0>)\n",
      "227 times loss: tensor(19.8493, grad_fn=<MseLossBackward0>)\n",
      "228 times loss: tensor(19.7572, grad_fn=<MseLossBackward0>)\n",
      "229 times loss: tensor(19.6661, grad_fn=<MseLossBackward0>)\n",
      "230 times loss: tensor(19.5761, grad_fn=<MseLossBackward0>)\n",
      "231 times loss: tensor(19.4870, grad_fn=<MseLossBackward0>)\n",
      "232 times loss: tensor(19.3990, grad_fn=<MseLossBackward0>)\n",
      "233 times loss: tensor(19.3119, grad_fn=<MseLossBackward0>)\n",
      "234 times loss: tensor(19.2258, grad_fn=<MseLossBackward0>)\n",
      "235 times loss: tensor(19.1406, grad_fn=<MseLossBackward0>)\n",
      "236 times loss: tensor(19.0563, grad_fn=<MseLossBackward0>)\n",
      "237 times loss: tensor(18.9729, grad_fn=<MseLossBackward0>)\n",
      "238 times loss: tensor(18.8904, grad_fn=<MseLossBackward0>)\n",
      "239 times loss: tensor(18.8088, grad_fn=<MseLossBackward0>)\n",
      "240 times loss: tensor(18.7280, grad_fn=<MseLossBackward0>)\n",
      "241 times loss: tensor(18.6482, grad_fn=<MseLossBackward0>)\n",
      "242 times loss: tensor(18.5691, grad_fn=<MseLossBackward0>)\n",
      "243 times loss: tensor(18.4910, grad_fn=<MseLossBackward0>)\n",
      "244 times loss: tensor(18.4136, grad_fn=<MseLossBackward0>)\n",
      "245 times loss: tensor(18.3370, grad_fn=<MseLossBackward0>)\n",
      "246 times loss: tensor(18.2612, grad_fn=<MseLossBackward0>)\n",
      "247 times loss: tensor(18.1862, grad_fn=<MseLossBackward0>)\n",
      "248 times loss: tensor(18.1120, grad_fn=<MseLossBackward0>)\n",
      "249 times loss: tensor(18.0384, grad_fn=<MseLossBackward0>)\n",
      "250 times loss: tensor(17.9657, grad_fn=<MseLossBackward0>)\n",
      "251 times loss: tensor(17.8936, grad_fn=<MseLossBackward0>)\n",
      "252 times loss: tensor(17.8222, grad_fn=<MseLossBackward0>)\n",
      "253 times loss: tensor(17.7516, grad_fn=<MseLossBackward0>)\n",
      "254 times loss: tensor(17.6816, grad_fn=<MseLossBackward0>)\n",
      "255 times loss: tensor(17.6123, grad_fn=<MseLossBackward0>)\n",
      "256 times loss: tensor(17.5438, grad_fn=<MseLossBackward0>)\n",
      "257 times loss: tensor(17.4759, grad_fn=<MseLossBackward0>)\n",
      "258 times loss: tensor(17.4086, grad_fn=<MseLossBackward0>)\n",
      "259 times loss: tensor(17.3420, grad_fn=<MseLossBackward0>)\n",
      "260 times loss: tensor(17.2760, grad_fn=<MseLossBackward0>)\n",
      "261 times loss: tensor(17.2106, grad_fn=<MseLossBackward0>)\n",
      "262 times loss: tensor(17.1458, grad_fn=<MseLossBackward0>)\n",
      "263 times loss: tensor(17.0817, grad_fn=<MseLossBackward0>)\n",
      "264 times loss: tensor(17.0181, grad_fn=<MseLossBackward0>)\n",
      "265 times loss: tensor(16.9551, grad_fn=<MseLossBackward0>)\n",
      "266 times loss: tensor(16.8927, grad_fn=<MseLossBackward0>)\n",
      "267 times loss: tensor(16.8309, grad_fn=<MseLossBackward0>)\n",
      "268 times loss: tensor(16.7696, grad_fn=<MseLossBackward0>)\n",
      "269 times loss: tensor(16.7089, grad_fn=<MseLossBackward0>)\n",
      "270 times loss: tensor(16.6487, grad_fn=<MseLossBackward0>)\n",
      "271 times loss: tensor(16.5890, grad_fn=<MseLossBackward0>)\n",
      "272 times loss: tensor(16.5299, grad_fn=<MseLossBackward0>)\n",
      "273 times loss: tensor(16.4713, grad_fn=<MseLossBackward0>)\n",
      "274 times loss: tensor(16.4132, grad_fn=<MseLossBackward0>)\n",
      "275 times loss: tensor(16.3556, grad_fn=<MseLossBackward0>)\n",
      "276 times loss: tensor(16.2985, grad_fn=<MseLossBackward0>)\n",
      "277 times loss: tensor(16.2420, grad_fn=<MseLossBackward0>)\n",
      "278 times loss: tensor(16.1859, grad_fn=<MseLossBackward0>)\n",
      "279 times loss: tensor(16.1302, grad_fn=<MseLossBackward0>)\n",
      "280 times loss: tensor(16.0751, grad_fn=<MseLossBackward0>)\n",
      "281 times loss: tensor(16.0204, grad_fn=<MseLossBackward0>)\n",
      "282 times loss: tensor(15.9662, grad_fn=<MseLossBackward0>)\n",
      "283 times loss: tensor(15.9124, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 times loss: tensor(15.8592, grad_fn=<MseLossBackward0>)\n",
      "285 times loss: tensor(15.8063, grad_fn=<MseLossBackward0>)\n",
      "286 times loss: tensor(15.7539, grad_fn=<MseLossBackward0>)\n",
      "287 times loss: tensor(15.7019, grad_fn=<MseLossBackward0>)\n",
      "288 times loss: tensor(15.6504, grad_fn=<MseLossBackward0>)\n",
      "289 times loss: tensor(15.5993, grad_fn=<MseLossBackward0>)\n",
      "290 times loss: tensor(15.5486, grad_fn=<MseLossBackward0>)\n",
      "291 times loss: tensor(15.4983, grad_fn=<MseLossBackward0>)\n",
      "292 times loss: tensor(15.4485, grad_fn=<MseLossBackward0>)\n",
      "293 times loss: tensor(15.3990, grad_fn=<MseLossBackward0>)\n",
      "294 times loss: tensor(15.3499, grad_fn=<MseLossBackward0>)\n",
      "295 times loss: tensor(15.3012, grad_fn=<MseLossBackward0>)\n",
      "296 times loss: tensor(15.2529, grad_fn=<MseLossBackward0>)\n",
      "297 times loss: tensor(15.2049, grad_fn=<MseLossBackward0>)\n",
      "298 times loss: tensor(15.1573, grad_fn=<MseLossBackward0>)\n",
      "299 times loss: tensor(15.1101, grad_fn=<MseLossBackward0>)\n",
      "300 times loss: tensor(15.0633, grad_fn=<MseLossBackward0>)\n",
      "301 times loss: tensor(15.0168, grad_fn=<MseLossBackward0>)\n",
      "302 times loss: tensor(14.9706, grad_fn=<MseLossBackward0>)\n",
      "303 times loss: tensor(14.9248, grad_fn=<MseLossBackward0>)\n",
      "304 times loss: tensor(14.8794, grad_fn=<MseLossBackward0>)\n",
      "305 times loss: tensor(14.8343, grad_fn=<MseLossBackward0>)\n",
      "306 times loss: tensor(14.7895, grad_fn=<MseLossBackward0>)\n",
      "307 times loss: tensor(14.7450, grad_fn=<MseLossBackward0>)\n",
      "308 times loss: tensor(14.7009, grad_fn=<MseLossBackward0>)\n",
      "309 times loss: tensor(14.6570, grad_fn=<MseLossBackward0>)\n",
      "310 times loss: tensor(14.6135, grad_fn=<MseLossBackward0>)\n",
      "311 times loss: tensor(14.5703, grad_fn=<MseLossBackward0>)\n",
      "312 times loss: tensor(14.5275, grad_fn=<MseLossBackward0>)\n",
      "313 times loss: tensor(14.4849, grad_fn=<MseLossBackward0>)\n",
      "314 times loss: tensor(14.4426, grad_fn=<MseLossBackward0>)\n",
      "315 times loss: tensor(14.4007, grad_fn=<MseLossBackward0>)\n",
      "316 times loss: tensor(14.3590, grad_fn=<MseLossBackward0>)\n",
      "317 times loss: tensor(14.3176, grad_fn=<MseLossBackward0>)\n",
      "318 times loss: tensor(14.2765, grad_fn=<MseLossBackward0>)\n",
      "319 times loss: tensor(14.2357, grad_fn=<MseLossBackward0>)\n",
      "320 times loss: tensor(14.1952, grad_fn=<MseLossBackward0>)\n",
      "321 times loss: tensor(14.1549, grad_fn=<MseLossBackward0>)\n",
      "322 times loss: tensor(14.1150, grad_fn=<MseLossBackward0>)\n",
      "323 times loss: tensor(14.0753, grad_fn=<MseLossBackward0>)\n",
      "324 times loss: tensor(14.0359, grad_fn=<MseLossBackward0>)\n",
      "325 times loss: tensor(13.9968, grad_fn=<MseLossBackward0>)\n",
      "326 times loss: tensor(13.9579, grad_fn=<MseLossBackward0>)\n",
      "327 times loss: tensor(13.9192, grad_fn=<MseLossBackward0>)\n",
      "328 times loss: tensor(13.8808, grad_fn=<MseLossBackward0>)\n",
      "329 times loss: tensor(13.8427, grad_fn=<MseLossBackward0>)\n",
      "330 times loss: tensor(13.8048, grad_fn=<MseLossBackward0>)\n",
      "331 times loss: tensor(13.7672, grad_fn=<MseLossBackward0>)\n",
      "332 times loss: tensor(13.7297, grad_fn=<MseLossBackward0>)\n",
      "333 times loss: tensor(13.6926, grad_fn=<MseLossBackward0>)\n",
      "334 times loss: tensor(13.6557, grad_fn=<MseLossBackward0>)\n",
      "335 times loss: tensor(13.6190, grad_fn=<MseLossBackward0>)\n",
      "336 times loss: tensor(13.5825, grad_fn=<MseLossBackward0>)\n",
      "337 times loss: tensor(13.5463, grad_fn=<MseLossBackward0>)\n",
      "338 times loss: tensor(13.5102, grad_fn=<MseLossBackward0>)\n",
      "339 times loss: tensor(13.4744, grad_fn=<MseLossBackward0>)\n",
      "340 times loss: tensor(13.4388, grad_fn=<MseLossBackward0>)\n",
      "341 times loss: tensor(13.4035, grad_fn=<MseLossBackward0>)\n",
      "342 times loss: tensor(13.3684, grad_fn=<MseLossBackward0>)\n",
      "343 times loss: tensor(13.3334, grad_fn=<MseLossBackward0>)\n",
      "344 times loss: tensor(13.2987, grad_fn=<MseLossBackward0>)\n",
      "345 times loss: tensor(13.2643, grad_fn=<MseLossBackward0>)\n",
      "346 times loss: tensor(13.2300, grad_fn=<MseLossBackward0>)\n",
      "347 times loss: tensor(13.1959, grad_fn=<MseLossBackward0>)\n",
      "348 times loss: tensor(13.1620, grad_fn=<MseLossBackward0>)\n",
      "349 times loss: tensor(13.1283, grad_fn=<MseLossBackward0>)\n",
      "350 times loss: tensor(13.0948, grad_fn=<MseLossBackward0>)\n",
      "351 times loss: tensor(13.0615, grad_fn=<MseLossBackward0>)\n",
      "352 times loss: tensor(13.0283, grad_fn=<MseLossBackward0>)\n",
      "353 times loss: tensor(12.9954, grad_fn=<MseLossBackward0>)\n",
      "354 times loss: tensor(12.9627, grad_fn=<MseLossBackward0>)\n",
      "355 times loss: tensor(12.9301, grad_fn=<MseLossBackward0>)\n",
      "356 times loss: tensor(12.8978, grad_fn=<MseLossBackward0>)\n",
      "357 times loss: tensor(12.8656, grad_fn=<MseLossBackward0>)\n",
      "358 times loss: tensor(12.8336, grad_fn=<MseLossBackward0>)\n",
      "359 times loss: tensor(12.8018, grad_fn=<MseLossBackward0>)\n",
      "360 times loss: tensor(12.7702, grad_fn=<MseLossBackward0>)\n",
      "361 times loss: tensor(12.7387, grad_fn=<MseLossBackward0>)\n",
      "362 times loss: tensor(12.7074, grad_fn=<MseLossBackward0>)\n",
      "363 times loss: tensor(12.6763, grad_fn=<MseLossBackward0>)\n",
      "364 times loss: tensor(12.6453, grad_fn=<MseLossBackward0>)\n",
      "365 times loss: tensor(12.6145, grad_fn=<MseLossBackward0>)\n",
      "366 times loss: tensor(12.5839, grad_fn=<MseLossBackward0>)\n",
      "367 times loss: tensor(12.5535, grad_fn=<MseLossBackward0>)\n",
      "368 times loss: tensor(12.5232, grad_fn=<MseLossBackward0>)\n",
      "369 times loss: tensor(12.4930, grad_fn=<MseLossBackward0>)\n",
      "370 times loss: tensor(12.4630, grad_fn=<MseLossBackward0>)\n",
      "371 times loss: tensor(12.4332, grad_fn=<MseLossBackward0>)\n",
      "372 times loss: tensor(12.4035, grad_fn=<MseLossBackward0>)\n",
      "373 times loss: tensor(12.3740, grad_fn=<MseLossBackward0>)\n",
      "374 times loss: tensor(12.3446, grad_fn=<MseLossBackward0>)\n",
      "375 times loss: tensor(12.3154, grad_fn=<MseLossBackward0>)\n",
      "376 times loss: tensor(12.2863, grad_fn=<MseLossBackward0>)\n",
      "377 times loss: tensor(12.2574, grad_fn=<MseLossBackward0>)\n",
      "378 times loss: tensor(12.2286, grad_fn=<MseLossBackward0>)\n",
      "379 times loss: tensor(12.2000, grad_fn=<MseLossBackward0>)\n",
      "380 times loss: tensor(12.1715, grad_fn=<MseLossBackward0>)\n",
      "381 times loss: tensor(12.1432, grad_fn=<MseLossBackward0>)\n",
      "382 times loss: tensor(12.1150, grad_fn=<MseLossBackward0>)\n",
      "383 times loss: tensor(12.0870, grad_fn=<MseLossBackward0>)\n",
      "384 times loss: tensor(12.0591, grad_fn=<MseLossBackward0>)\n",
      "385 times loss: tensor(12.0314, grad_fn=<MseLossBackward0>)\n",
      "386 times loss: tensor(12.0038, grad_fn=<MseLossBackward0>)\n",
      "387 times loss: tensor(11.9763, grad_fn=<MseLossBackward0>)\n",
      "388 times loss: tensor(11.9490, grad_fn=<MseLossBackward0>)\n",
      "389 times loss: tensor(11.9218, grad_fn=<MseLossBackward0>)\n",
      "390 times loss: tensor(11.8947, grad_fn=<MseLossBackward0>)\n",
      "391 times loss: tensor(11.8678, grad_fn=<MseLossBackward0>)\n",
      "392 times loss: tensor(11.8410, grad_fn=<MseLossBackward0>)\n",
      "393 times loss: tensor(11.8143, grad_fn=<MseLossBackward0>)\n",
      "394 times loss: tensor(11.7878, grad_fn=<MseLossBackward0>)\n",
      "395 times loss: tensor(11.7614, grad_fn=<MseLossBackward0>)\n",
      "396 times loss: tensor(11.7351, grad_fn=<MseLossBackward0>)\n",
      "397 times loss: tensor(11.7089, grad_fn=<MseLossBackward0>)\n",
      "398 times loss: tensor(11.6829, grad_fn=<MseLossBackward0>)\n",
      "399 times loss: tensor(11.6569, grad_fn=<MseLossBackward0>)\n",
      "400 times loss: tensor(11.6311, grad_fn=<MseLossBackward0>)\n",
      "401 times loss: tensor(11.6054, grad_fn=<MseLossBackward0>)\n",
      "402 times loss: tensor(11.5799, grad_fn=<MseLossBackward0>)\n",
      "403 times loss: tensor(11.5544, grad_fn=<MseLossBackward0>)\n",
      "404 times loss: tensor(11.5291, grad_fn=<MseLossBackward0>)\n",
      "405 times loss: tensor(11.5039, grad_fn=<MseLossBackward0>)\n",
      "406 times loss: tensor(11.4788, grad_fn=<MseLossBackward0>)\n",
      "407 times loss: tensor(11.4538, grad_fn=<MseLossBackward0>)\n",
      "408 times loss: tensor(11.4289, grad_fn=<MseLossBackward0>)\n",
      "409 times loss: tensor(11.4042, grad_fn=<MseLossBackward0>)\n",
      "410 times loss: tensor(11.3795, grad_fn=<MseLossBackward0>)\n",
      "411 times loss: tensor(11.3550, grad_fn=<MseLossBackward0>)\n",
      "412 times loss: tensor(11.3306, grad_fn=<MseLossBackward0>)\n",
      "413 times loss: tensor(11.3063, grad_fn=<MseLossBackward0>)\n",
      "414 times loss: tensor(11.2821, grad_fn=<MseLossBackward0>)\n",
      "415 times loss: tensor(11.2580, grad_fn=<MseLossBackward0>)\n",
      "416 times loss: tensor(11.2340, grad_fn=<MseLossBackward0>)\n",
      "417 times loss: tensor(11.2101, grad_fn=<MseLossBackward0>)\n",
      "418 times loss: tensor(11.1863, grad_fn=<MseLossBackward0>)\n",
      "419 times loss: tensor(11.1627, grad_fn=<MseLossBackward0>)\n",
      "420 times loss: tensor(11.1391, grad_fn=<MseLossBackward0>)\n",
      "421 times loss: tensor(11.1156, grad_fn=<MseLossBackward0>)\n",
      "422 times loss: tensor(11.0923, grad_fn=<MseLossBackward0>)\n",
      "423 times loss: tensor(11.0690, grad_fn=<MseLossBackward0>)\n",
      "424 times loss: tensor(11.0458, grad_fn=<MseLossBackward0>)\n",
      "425 times loss: tensor(11.0227, grad_fn=<MseLossBackward0>)\n",
      "426 times loss: tensor(10.9997, grad_fn=<MseLossBackward0>)\n",
      "427 times loss: tensor(10.9768, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 times loss: tensor(10.9541, grad_fn=<MseLossBackward0>)\n",
      "429 times loss: tensor(10.9314, grad_fn=<MseLossBackward0>)\n",
      "430 times loss: tensor(10.9088, grad_fn=<MseLossBackward0>)\n",
      "431 times loss: tensor(10.8863, grad_fn=<MseLossBackward0>)\n",
      "432 times loss: tensor(10.8639, grad_fn=<MseLossBackward0>)\n",
      "433 times loss: tensor(10.8415, grad_fn=<MseLossBackward0>)\n",
      "434 times loss: tensor(10.8193, grad_fn=<MseLossBackward0>)\n",
      "435 times loss: tensor(10.7972, grad_fn=<MseLossBackward0>)\n",
      "436 times loss: tensor(10.7751, grad_fn=<MseLossBackward0>)\n",
      "437 times loss: tensor(10.7531, grad_fn=<MseLossBackward0>)\n",
      "438 times loss: tensor(10.7313, grad_fn=<MseLossBackward0>)\n",
      "439 times loss: tensor(10.7095, grad_fn=<MseLossBackward0>)\n",
      "440 times loss: tensor(10.6878, grad_fn=<MseLossBackward0>)\n",
      "441 times loss: tensor(10.6662, grad_fn=<MseLossBackward0>)\n",
      "442 times loss: tensor(10.6447, grad_fn=<MseLossBackward0>)\n",
      "443 times loss: tensor(10.6233, grad_fn=<MseLossBackward0>)\n",
      "444 times loss: tensor(10.6020, grad_fn=<MseLossBackward0>)\n",
      "445 times loss: tensor(10.5807, grad_fn=<MseLossBackward0>)\n",
      "446 times loss: tensor(10.5596, grad_fn=<MseLossBackward0>)\n",
      "447 times loss: tensor(10.5385, grad_fn=<MseLossBackward0>)\n",
      "448 times loss: tensor(10.5175, grad_fn=<MseLossBackward0>)\n",
      "449 times loss: tensor(10.4966, grad_fn=<MseLossBackward0>)\n",
      "450 times loss: tensor(10.4758, grad_fn=<MseLossBackward0>)\n",
      "451 times loss: tensor(10.4550, grad_fn=<MseLossBackward0>)\n",
      "452 times loss: tensor(10.4343, grad_fn=<MseLossBackward0>)\n",
      "453 times loss: tensor(10.4137, grad_fn=<MseLossBackward0>)\n",
      "454 times loss: tensor(10.3932, grad_fn=<MseLossBackward0>)\n",
      "455 times loss: tensor(10.3728, grad_fn=<MseLossBackward0>)\n",
      "456 times loss: tensor(10.3524, grad_fn=<MseLossBackward0>)\n",
      "457 times loss: tensor(10.3321, grad_fn=<MseLossBackward0>)\n",
      "458 times loss: tensor(10.3119, grad_fn=<MseLossBackward0>)\n",
      "459 times loss: tensor(10.2917, grad_fn=<MseLossBackward0>)\n",
      "460 times loss: tensor(10.2717, grad_fn=<MseLossBackward0>)\n",
      "461 times loss: tensor(10.2517, grad_fn=<MseLossBackward0>)\n",
      "462 times loss: tensor(10.2318, grad_fn=<MseLossBackward0>)\n",
      "463 times loss: tensor(10.2120, grad_fn=<MseLossBackward0>)\n",
      "464 times loss: tensor(10.1922, grad_fn=<MseLossBackward0>)\n",
      "465 times loss: tensor(10.1725, grad_fn=<MseLossBackward0>)\n",
      "466 times loss: tensor(10.1529, grad_fn=<MseLossBackward0>)\n",
      "467 times loss: tensor(10.1334, grad_fn=<MseLossBackward0>)\n",
      "468 times loss: tensor(10.1139, grad_fn=<MseLossBackward0>)\n",
      "469 times loss: tensor(10.0945, grad_fn=<MseLossBackward0>)\n",
      "470 times loss: tensor(10.0752, grad_fn=<MseLossBackward0>)\n",
      "471 times loss: tensor(10.0560, grad_fn=<MseLossBackward0>)\n",
      "472 times loss: tensor(10.0368, grad_fn=<MseLossBackward0>)\n",
      "473 times loss: tensor(10.0177, grad_fn=<MseLossBackward0>)\n",
      "474 times loss: tensor(9.9987, grad_fn=<MseLossBackward0>)\n",
      "475 times loss: tensor(9.9797, grad_fn=<MseLossBackward0>)\n",
      "476 times loss: tensor(9.9609, grad_fn=<MseLossBackward0>)\n",
      "477 times loss: tensor(9.9421, grad_fn=<MseLossBackward0>)\n",
      "478 times loss: tensor(9.9233, grad_fn=<MseLossBackward0>)\n",
      "479 times loss: tensor(9.9047, grad_fn=<MseLossBackward0>)\n",
      "480 times loss: tensor(9.8861, grad_fn=<MseLossBackward0>)\n",
      "481 times loss: tensor(9.8675, grad_fn=<MseLossBackward0>)\n",
      "482 times loss: tensor(9.8491, grad_fn=<MseLossBackward0>)\n",
      "483 times loss: tensor(9.8307, grad_fn=<MseLossBackward0>)\n",
      "484 times loss: tensor(9.8124, grad_fn=<MseLossBackward0>)\n",
      "485 times loss: tensor(9.7941, grad_fn=<MseLossBackward0>)\n",
      "486 times loss: tensor(9.7759, grad_fn=<MseLossBackward0>)\n",
      "487 times loss: tensor(9.7578, grad_fn=<MseLossBackward0>)\n",
      "488 times loss: tensor(9.7398, grad_fn=<MseLossBackward0>)\n",
      "489 times loss: tensor(9.7218, grad_fn=<MseLossBackward0>)\n",
      "490 times loss: tensor(9.7038, grad_fn=<MseLossBackward0>)\n",
      "491 times loss: tensor(9.6860, grad_fn=<MseLossBackward0>)\n",
      "492 times loss: tensor(9.6682, grad_fn=<MseLossBackward0>)\n",
      "493 times loss: tensor(9.6504, grad_fn=<MseLossBackward0>)\n",
      "494 times loss: tensor(9.6327, grad_fn=<MseLossBackward0>)\n",
      "495 times loss: tensor(9.6151, grad_fn=<MseLossBackward0>)\n",
      "496 times loss: tensor(9.5975, grad_fn=<MseLossBackward0>)\n",
      "497 times loss: tensor(9.5801, grad_fn=<MseLossBackward0>)\n",
      "498 times loss: tensor(9.5626, grad_fn=<MseLossBackward0>)\n",
      "499 times loss: tensor(9.5452, grad_fn=<MseLossBackward0>)\n",
      "500 times loss: tensor(9.5279, grad_fn=<MseLossBackward0>)\n",
      "501 times loss: tensor(9.5107, grad_fn=<MseLossBackward0>)\n",
      "502 times loss: tensor(9.4935, grad_fn=<MseLossBackward0>)\n",
      "503 times loss: tensor(9.4763, grad_fn=<MseLossBackward0>)\n",
      "504 times loss: tensor(9.4592, grad_fn=<MseLossBackward0>)\n",
      "505 times loss: tensor(9.4422, grad_fn=<MseLossBackward0>)\n",
      "506 times loss: tensor(9.4253, grad_fn=<MseLossBackward0>)\n",
      "507 times loss: tensor(9.4084, grad_fn=<MseLossBackward0>)\n",
      "508 times loss: tensor(9.3915, grad_fn=<MseLossBackward0>)\n",
      "509 times loss: tensor(9.3747, grad_fn=<MseLossBackward0>)\n",
      "510 times loss: tensor(9.3580, grad_fn=<MseLossBackward0>)\n",
      "511 times loss: tensor(9.3413, grad_fn=<MseLossBackward0>)\n",
      "512 times loss: tensor(9.3247, grad_fn=<MseLossBackward0>)\n",
      "513 times loss: tensor(9.3081, grad_fn=<MseLossBackward0>)\n",
      "514 times loss: tensor(9.2916, grad_fn=<MseLossBackward0>)\n",
      "515 times loss: tensor(9.2752, grad_fn=<MseLossBackward0>)\n",
      "516 times loss: tensor(9.2588, grad_fn=<MseLossBackward0>)\n",
      "517 times loss: tensor(9.2424, grad_fn=<MseLossBackward0>)\n",
      "518 times loss: tensor(9.2261, grad_fn=<MseLossBackward0>)\n",
      "519 times loss: tensor(9.2099, grad_fn=<MseLossBackward0>)\n",
      "520 times loss: tensor(9.1937, grad_fn=<MseLossBackward0>)\n",
      "521 times loss: tensor(9.1775, grad_fn=<MseLossBackward0>)\n",
      "522 times loss: tensor(9.1614, grad_fn=<MseLossBackward0>)\n",
      "523 times loss: tensor(9.1454, grad_fn=<MseLossBackward0>)\n",
      "524 times loss: tensor(9.1294, grad_fn=<MseLossBackward0>)\n",
      "525 times loss: tensor(9.1135, grad_fn=<MseLossBackward0>)\n",
      "526 times loss: tensor(9.0976, grad_fn=<MseLossBackward0>)\n",
      "527 times loss: tensor(9.0818, grad_fn=<MseLossBackward0>)\n",
      "528 times loss: tensor(9.0660, grad_fn=<MseLossBackward0>)\n",
      "529 times loss: tensor(9.0503, grad_fn=<MseLossBackward0>)\n",
      "530 times loss: tensor(9.0346, grad_fn=<MseLossBackward0>)\n",
      "531 times loss: tensor(9.0190, grad_fn=<MseLossBackward0>)\n",
      "532 times loss: tensor(9.0035, grad_fn=<MseLossBackward0>)\n",
      "533 times loss: tensor(8.9879, grad_fn=<MseLossBackward0>)\n",
      "534 times loss: tensor(8.9725, grad_fn=<MseLossBackward0>)\n",
      "535 times loss: tensor(8.9571, grad_fn=<MseLossBackward0>)\n",
      "536 times loss: tensor(8.9417, grad_fn=<MseLossBackward0>)\n",
      "537 times loss: tensor(8.9264, grad_fn=<MseLossBackward0>)\n",
      "538 times loss: tensor(8.9111, grad_fn=<MseLossBackward0>)\n",
      "539 times loss: tensor(8.8959, grad_fn=<MseLossBackward0>)\n",
      "540 times loss: tensor(8.8807, grad_fn=<MseLossBackward0>)\n",
      "541 times loss: tensor(8.8656, grad_fn=<MseLossBackward0>)\n",
      "542 times loss: tensor(8.8505, grad_fn=<MseLossBackward0>)\n",
      "543 times loss: tensor(8.8355, grad_fn=<MseLossBackward0>)\n",
      "544 times loss: tensor(8.8206, grad_fn=<MseLossBackward0>)\n",
      "545 times loss: tensor(8.8056, grad_fn=<MseLossBackward0>)\n",
      "546 times loss: tensor(8.7908, grad_fn=<MseLossBackward0>)\n",
      "547 times loss: tensor(8.7760, grad_fn=<MseLossBackward0>)\n",
      "548 times loss: tensor(8.7612, grad_fn=<MseLossBackward0>)\n",
      "549 times loss: tensor(8.7465, grad_fn=<MseLossBackward0>)\n",
      "550 times loss: tensor(8.7318, grad_fn=<MseLossBackward0>)\n",
      "551 times loss: tensor(8.7172, grad_fn=<MseLossBackward0>)\n",
      "552 times loss: tensor(8.7026, grad_fn=<MseLossBackward0>)\n",
      "553 times loss: tensor(8.6880, grad_fn=<MseLossBackward0>)\n",
      "554 times loss: tensor(8.6735, grad_fn=<MseLossBackward0>)\n",
      "555 times loss: tensor(8.6591, grad_fn=<MseLossBackward0>)\n",
      "556 times loss: tensor(8.6447, grad_fn=<MseLossBackward0>)\n",
      "557 times loss: tensor(8.6303, grad_fn=<MseLossBackward0>)\n",
      "558 times loss: tensor(8.6160, grad_fn=<MseLossBackward0>)\n",
      "559 times loss: tensor(8.6017, grad_fn=<MseLossBackward0>)\n",
      "560 times loss: tensor(8.5875, grad_fn=<MseLossBackward0>)\n",
      "561 times loss: tensor(8.5733, grad_fn=<MseLossBackward0>)\n",
      "562 times loss: tensor(8.5591, grad_fn=<MseLossBackward0>)\n",
      "563 times loss: tensor(8.5450, grad_fn=<MseLossBackward0>)\n",
      "564 times loss: tensor(8.5310, grad_fn=<MseLossBackward0>)\n",
      "565 times loss: tensor(8.5169, grad_fn=<MseLossBackward0>)\n",
      "566 times loss: tensor(8.5029, grad_fn=<MseLossBackward0>)\n",
      "567 times loss: tensor(8.4890, grad_fn=<MseLossBackward0>)\n",
      "568 times loss: tensor(8.4751, grad_fn=<MseLossBackward0>)\n",
      "569 times loss: tensor(8.4612, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570 times loss: tensor(8.4474, grad_fn=<MseLossBackward0>)\n",
      "571 times loss: tensor(8.4336, grad_fn=<MseLossBackward0>)\n",
      "572 times loss: tensor(8.4199, grad_fn=<MseLossBackward0>)\n",
      "573 times loss: tensor(8.4062, grad_fn=<MseLossBackward0>)\n",
      "574 times loss: tensor(8.3925, grad_fn=<MseLossBackward0>)\n",
      "575 times loss: tensor(8.3789, grad_fn=<MseLossBackward0>)\n",
      "576 times loss: tensor(8.3654, grad_fn=<MseLossBackward0>)\n",
      "577 times loss: tensor(8.3518, grad_fn=<MseLossBackward0>)\n",
      "578 times loss: tensor(8.3383, grad_fn=<MseLossBackward0>)\n",
      "579 times loss: tensor(8.3249, grad_fn=<MseLossBackward0>)\n",
      "580 times loss: tensor(8.3115, grad_fn=<MseLossBackward0>)\n",
      "581 times loss: tensor(8.2981, grad_fn=<MseLossBackward0>)\n",
      "582 times loss: tensor(8.2848, grad_fn=<MseLossBackward0>)\n",
      "583 times loss: tensor(8.2715, grad_fn=<MseLossBackward0>)\n",
      "584 times loss: tensor(8.2582, grad_fn=<MseLossBackward0>)\n",
      "585 times loss: tensor(8.2450, grad_fn=<MseLossBackward0>)\n",
      "586 times loss: tensor(8.2319, grad_fn=<MseLossBackward0>)\n",
      "587 times loss: tensor(8.2187, grad_fn=<MseLossBackward0>)\n",
      "588 times loss: tensor(8.2056, grad_fn=<MseLossBackward0>)\n",
      "589 times loss: tensor(8.1926, grad_fn=<MseLossBackward0>)\n",
      "590 times loss: tensor(8.1796, grad_fn=<MseLossBackward0>)\n",
      "591 times loss: tensor(8.1666, grad_fn=<MseLossBackward0>)\n",
      "592 times loss: tensor(8.1537, grad_fn=<MseLossBackward0>)\n",
      "593 times loss: tensor(8.1408, grad_fn=<MseLossBackward0>)\n",
      "594 times loss: tensor(8.1279, grad_fn=<MseLossBackward0>)\n",
      "595 times loss: tensor(8.1151, grad_fn=<MseLossBackward0>)\n",
      "596 times loss: tensor(8.1023, grad_fn=<MseLossBackward0>)\n",
      "597 times loss: tensor(8.0895, grad_fn=<MseLossBackward0>)\n",
      "598 times loss: tensor(8.0768, grad_fn=<MseLossBackward0>)\n",
      "599 times loss: tensor(8.0642, grad_fn=<MseLossBackward0>)\n",
      "600 times loss: tensor(8.0515, grad_fn=<MseLossBackward0>)\n",
      "601 times loss: tensor(8.0389, grad_fn=<MseLossBackward0>)\n",
      "602 times loss: tensor(8.0264, grad_fn=<MseLossBackward0>)\n",
      "603 times loss: tensor(8.0139, grad_fn=<MseLossBackward0>)\n",
      "604 times loss: tensor(8.0014, grad_fn=<MseLossBackward0>)\n",
      "605 times loss: tensor(7.9889, grad_fn=<MseLossBackward0>)\n",
      "606 times loss: tensor(7.9765, grad_fn=<MseLossBackward0>)\n",
      "607 times loss: tensor(7.9641, grad_fn=<MseLossBackward0>)\n",
      "608 times loss: tensor(7.9518, grad_fn=<MseLossBackward0>)\n",
      "609 times loss: tensor(7.9395, grad_fn=<MseLossBackward0>)\n",
      "610 times loss: tensor(7.9272, grad_fn=<MseLossBackward0>)\n",
      "611 times loss: tensor(7.9150, grad_fn=<MseLossBackward0>)\n",
      "612 times loss: tensor(7.9027, grad_fn=<MseLossBackward0>)\n",
      "613 times loss: tensor(7.8906, grad_fn=<MseLossBackward0>)\n",
      "614 times loss: tensor(7.8784, grad_fn=<MseLossBackward0>)\n",
      "615 times loss: tensor(7.8663, grad_fn=<MseLossBackward0>)\n",
      "616 times loss: tensor(7.8543, grad_fn=<MseLossBackward0>)\n",
      "617 times loss: tensor(7.8422, grad_fn=<MseLossBackward0>)\n",
      "618 times loss: tensor(7.8302, grad_fn=<MseLossBackward0>)\n",
      "619 times loss: tensor(7.8183, grad_fn=<MseLossBackward0>)\n",
      "620 times loss: tensor(7.8063, grad_fn=<MseLossBackward0>)\n",
      "621 times loss: tensor(7.7944, grad_fn=<MseLossBackward0>)\n",
      "622 times loss: tensor(7.7825, grad_fn=<MseLossBackward0>)\n",
      "623 times loss: tensor(7.7707, grad_fn=<MseLossBackward0>)\n",
      "624 times loss: tensor(7.7589, grad_fn=<MseLossBackward0>)\n",
      "625 times loss: tensor(7.7471, grad_fn=<MseLossBackward0>)\n",
      "626 times loss: tensor(7.7354, grad_fn=<MseLossBackward0>)\n",
      "627 times loss: tensor(7.7237, grad_fn=<MseLossBackward0>)\n",
      "628 times loss: tensor(7.7120, grad_fn=<MseLossBackward0>)\n",
      "629 times loss: tensor(7.7004, grad_fn=<MseLossBackward0>)\n",
      "630 times loss: tensor(7.6888, grad_fn=<MseLossBackward0>)\n",
      "631 times loss: tensor(7.6772, grad_fn=<MseLossBackward0>)\n",
      "632 times loss: tensor(7.6657, grad_fn=<MseLossBackward0>)\n",
      "633 times loss: tensor(7.6542, grad_fn=<MseLossBackward0>)\n",
      "634 times loss: tensor(7.6427, grad_fn=<MseLossBackward0>)\n",
      "635 times loss: tensor(7.6313, grad_fn=<MseLossBackward0>)\n",
      "636 times loss: tensor(7.6198, grad_fn=<MseLossBackward0>)\n",
      "637 times loss: tensor(7.6085, grad_fn=<MseLossBackward0>)\n",
      "638 times loss: tensor(7.5971, grad_fn=<MseLossBackward0>)\n",
      "639 times loss: tensor(7.5858, grad_fn=<MseLossBackward0>)\n",
      "640 times loss: tensor(7.5745, grad_fn=<MseLossBackward0>)\n",
      "641 times loss: tensor(7.5632, grad_fn=<MseLossBackward0>)\n",
      "642 times loss: tensor(7.5520, grad_fn=<MseLossBackward0>)\n",
      "643 times loss: tensor(7.5408, grad_fn=<MseLossBackward0>)\n",
      "644 times loss: tensor(7.5296, grad_fn=<MseLossBackward0>)\n",
      "645 times loss: tensor(7.5185, grad_fn=<MseLossBackward0>)\n",
      "646 times loss: tensor(7.5074, grad_fn=<MseLossBackward0>)\n",
      "647 times loss: tensor(7.4963, grad_fn=<MseLossBackward0>)\n",
      "648 times loss: tensor(7.4852, grad_fn=<MseLossBackward0>)\n",
      "649 times loss: tensor(7.4742, grad_fn=<MseLossBackward0>)\n",
      "650 times loss: tensor(7.4632, grad_fn=<MseLossBackward0>)\n",
      "651 times loss: tensor(7.4523, grad_fn=<MseLossBackward0>)\n",
      "652 times loss: tensor(7.4413, grad_fn=<MseLossBackward0>)\n",
      "653 times loss: tensor(7.4304, grad_fn=<MseLossBackward0>)\n",
      "654 times loss: tensor(7.4196, grad_fn=<MseLossBackward0>)\n",
      "655 times loss: tensor(7.4087, grad_fn=<MseLossBackward0>)\n",
      "656 times loss: tensor(7.3979, grad_fn=<MseLossBackward0>)\n",
      "657 times loss: tensor(7.3871, grad_fn=<MseLossBackward0>)\n",
      "658 times loss: tensor(7.3764, grad_fn=<MseLossBackward0>)\n",
      "659 times loss: tensor(7.3656, grad_fn=<MseLossBackward0>)\n",
      "660 times loss: tensor(7.3550, grad_fn=<MseLossBackward0>)\n",
      "661 times loss: tensor(7.3443, grad_fn=<MseLossBackward0>)\n",
      "662 times loss: tensor(7.3337, grad_fn=<MseLossBackward0>)\n",
      "663 times loss: tensor(7.3230, grad_fn=<MseLossBackward0>)\n",
      "664 times loss: tensor(7.3125, grad_fn=<MseLossBackward0>)\n",
      "665 times loss: tensor(7.3019, grad_fn=<MseLossBackward0>)\n",
      "666 times loss: tensor(7.2914, grad_fn=<MseLossBackward0>)\n",
      "667 times loss: tensor(7.2809, grad_fn=<MseLossBackward0>)\n",
      "668 times loss: tensor(7.2704, grad_fn=<MseLossBackward0>)\n",
      "669 times loss: tensor(7.2600, grad_fn=<MseLossBackward0>)\n",
      "670 times loss: tensor(7.2496, grad_fn=<MseLossBackward0>)\n",
      "671 times loss: tensor(7.2392, grad_fn=<MseLossBackward0>)\n",
      "672 times loss: tensor(7.2288, grad_fn=<MseLossBackward0>)\n",
      "673 times loss: tensor(7.2185, grad_fn=<MseLossBackward0>)\n",
      "674 times loss: tensor(7.2082, grad_fn=<MseLossBackward0>)\n",
      "675 times loss: tensor(7.1979, grad_fn=<MseLossBackward0>)\n",
      "676 times loss: tensor(7.1877, grad_fn=<MseLossBackward0>)\n",
      "677 times loss: tensor(7.1774, grad_fn=<MseLossBackward0>)\n",
      "678 times loss: tensor(7.1672, grad_fn=<MseLossBackward0>)\n",
      "679 times loss: tensor(7.1571, grad_fn=<MseLossBackward0>)\n",
      "680 times loss: tensor(7.1469, grad_fn=<MseLossBackward0>)\n",
      "681 times loss: tensor(7.1368, grad_fn=<MseLossBackward0>)\n",
      "682 times loss: tensor(7.1267, grad_fn=<MseLossBackward0>)\n",
      "683 times loss: tensor(7.1167, grad_fn=<MseLossBackward0>)\n",
      "684 times loss: tensor(7.1066, grad_fn=<MseLossBackward0>)\n",
      "685 times loss: tensor(7.0966, grad_fn=<MseLossBackward0>)\n",
      "686 times loss: tensor(7.0866, grad_fn=<MseLossBackward0>)\n",
      "687 times loss: tensor(7.0767, grad_fn=<MseLossBackward0>)\n",
      "688 times loss: tensor(7.0667, grad_fn=<MseLossBackward0>)\n",
      "689 times loss: tensor(7.0568, grad_fn=<MseLossBackward0>)\n",
      "690 times loss: tensor(7.0469, grad_fn=<MseLossBackward0>)\n",
      "691 times loss: tensor(7.0371, grad_fn=<MseLossBackward0>)\n",
      "692 times loss: tensor(7.0273, grad_fn=<MseLossBackward0>)\n",
      "693 times loss: tensor(7.0175, grad_fn=<MseLossBackward0>)\n",
      "694 times loss: tensor(7.0077, grad_fn=<MseLossBackward0>)\n",
      "695 times loss: tensor(6.9979, grad_fn=<MseLossBackward0>)\n",
      "696 times loss: tensor(6.9882, grad_fn=<MseLossBackward0>)\n",
      "697 times loss: tensor(6.9785, grad_fn=<MseLossBackward0>)\n",
      "698 times loss: tensor(6.9688, grad_fn=<MseLossBackward0>)\n",
      "699 times loss: tensor(6.9591, grad_fn=<MseLossBackward0>)\n",
      "700 times loss: tensor(6.9495, grad_fn=<MseLossBackward0>)\n",
      "701 times loss: tensor(6.9399, grad_fn=<MseLossBackward0>)\n",
      "702 times loss: tensor(6.9303, grad_fn=<MseLossBackward0>)\n",
      "703 times loss: tensor(6.9208, grad_fn=<MseLossBackward0>)\n",
      "704 times loss: tensor(6.9112, grad_fn=<MseLossBackward0>)\n",
      "705 times loss: tensor(6.9017, grad_fn=<MseLossBackward0>)\n",
      "706 times loss: tensor(6.8922, grad_fn=<MseLossBackward0>)\n",
      "707 times loss: tensor(6.8828, grad_fn=<MseLossBackward0>)\n",
      "708 times loss: tensor(6.8733, grad_fn=<MseLossBackward0>)\n",
      "709 times loss: tensor(6.8639, grad_fn=<MseLossBackward0>)\n",
      "710 times loss: tensor(6.8546, grad_fn=<MseLossBackward0>)\n",
      "711 times loss: tensor(6.8452, grad_fn=<MseLossBackward0>)\n",
      "712 times loss: tensor(6.8358, grad_fn=<MseLossBackward0>)\n",
      "713 times loss: tensor(6.8265, grad_fn=<MseLossBackward0>)\n",
      "714 times loss: tensor(6.8172, grad_fn=<MseLossBackward0>)\n",
      "715 times loss: tensor(6.8080, grad_fn=<MseLossBackward0>)\n",
      "716 times loss: tensor(6.7987, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717 times loss: tensor(6.7895, grad_fn=<MseLossBackward0>)\n",
      "718 times loss: tensor(6.7803, grad_fn=<MseLossBackward0>)\n",
      "719 times loss: tensor(6.7711, grad_fn=<MseLossBackward0>)\n",
      "720 times loss: tensor(6.7620, grad_fn=<MseLossBackward0>)\n",
      "721 times loss: tensor(6.7529, grad_fn=<MseLossBackward0>)\n",
      "722 times loss: tensor(6.7438, grad_fn=<MseLossBackward0>)\n",
      "723 times loss: tensor(6.7347, grad_fn=<MseLossBackward0>)\n",
      "724 times loss: tensor(6.7256, grad_fn=<MseLossBackward0>)\n",
      "725 times loss: tensor(6.7166, grad_fn=<MseLossBackward0>)\n",
      "726 times loss: tensor(6.7076, grad_fn=<MseLossBackward0>)\n",
      "727 times loss: tensor(6.6986, grad_fn=<MseLossBackward0>)\n",
      "728 times loss: tensor(6.6896, grad_fn=<MseLossBackward0>)\n",
      "729 times loss: tensor(6.6807, grad_fn=<MseLossBackward0>)\n",
      "730 times loss: tensor(6.6717, grad_fn=<MseLossBackward0>)\n",
      "731 times loss: tensor(6.6628, grad_fn=<MseLossBackward0>)\n",
      "732 times loss: tensor(6.6540, grad_fn=<MseLossBackward0>)\n",
      "733 times loss: tensor(6.6451, grad_fn=<MseLossBackward0>)\n",
      "734 times loss: tensor(6.6363, grad_fn=<MseLossBackward0>)\n",
      "735 times loss: tensor(6.6274, grad_fn=<MseLossBackward0>)\n",
      "736 times loss: tensor(6.6187, grad_fn=<MseLossBackward0>)\n",
      "737 times loss: tensor(6.6099, grad_fn=<MseLossBackward0>)\n",
      "738 times loss: tensor(6.6011, grad_fn=<MseLossBackward0>)\n",
      "739 times loss: tensor(6.5924, grad_fn=<MseLossBackward0>)\n",
      "740 times loss: tensor(6.5837, grad_fn=<MseLossBackward0>)\n",
      "741 times loss: tensor(6.5750, grad_fn=<MseLossBackward0>)\n",
      "742 times loss: tensor(6.5664, grad_fn=<MseLossBackward0>)\n",
      "743 times loss: tensor(6.5577, grad_fn=<MseLossBackward0>)\n",
      "744 times loss: tensor(6.5491, grad_fn=<MseLossBackward0>)\n",
      "745 times loss: tensor(6.5405, grad_fn=<MseLossBackward0>)\n",
      "746 times loss: tensor(6.5319, grad_fn=<MseLossBackward0>)\n",
      "747 times loss: tensor(6.5234, grad_fn=<MseLossBackward0>)\n",
      "748 times loss: tensor(6.5148, grad_fn=<MseLossBackward0>)\n",
      "749 times loss: tensor(6.5063, grad_fn=<MseLossBackward0>)\n",
      "750 times loss: tensor(6.4978, grad_fn=<MseLossBackward0>)\n",
      "751 times loss: tensor(6.4893, grad_fn=<MseLossBackward0>)\n",
      "752 times loss: tensor(6.4809, grad_fn=<MseLossBackward0>)\n",
      "753 times loss: tensor(6.4724, grad_fn=<MseLossBackward0>)\n",
      "754 times loss: tensor(6.4640, grad_fn=<MseLossBackward0>)\n",
      "755 times loss: tensor(6.4556, grad_fn=<MseLossBackward0>)\n",
      "756 times loss: tensor(6.4472, grad_fn=<MseLossBackward0>)\n",
      "757 times loss: tensor(6.4389, grad_fn=<MseLossBackward0>)\n",
      "758 times loss: tensor(6.4305, grad_fn=<MseLossBackward0>)\n",
      "759 times loss: tensor(6.4222, grad_fn=<MseLossBackward0>)\n",
      "760 times loss: tensor(6.4139, grad_fn=<MseLossBackward0>)\n",
      "761 times loss: tensor(6.4056, grad_fn=<MseLossBackward0>)\n",
      "762 times loss: tensor(6.3973, grad_fn=<MseLossBackward0>)\n",
      "763 times loss: tensor(6.3891, grad_fn=<MseLossBackward0>)\n",
      "764 times loss: tensor(6.3809, grad_fn=<MseLossBackward0>)\n",
      "765 times loss: tensor(6.3727, grad_fn=<MseLossBackward0>)\n",
      "766 times loss: tensor(6.3645, grad_fn=<MseLossBackward0>)\n",
      "767 times loss: tensor(6.3563, grad_fn=<MseLossBackward0>)\n",
      "768 times loss: tensor(6.3482, grad_fn=<MseLossBackward0>)\n",
      "769 times loss: tensor(6.3401, grad_fn=<MseLossBackward0>)\n",
      "770 times loss: tensor(6.3320, grad_fn=<MseLossBackward0>)\n",
      "771 times loss: tensor(6.3239, grad_fn=<MseLossBackward0>)\n",
      "772 times loss: tensor(6.3158, grad_fn=<MseLossBackward0>)\n",
      "773 times loss: tensor(6.3078, grad_fn=<MseLossBackward0>)\n",
      "774 times loss: tensor(6.2997, grad_fn=<MseLossBackward0>)\n",
      "775 times loss: tensor(6.2917, grad_fn=<MseLossBackward0>)\n",
      "776 times loss: tensor(6.2837, grad_fn=<MseLossBackward0>)\n",
      "777 times loss: tensor(6.2758, grad_fn=<MseLossBackward0>)\n",
      "778 times loss: tensor(6.2678, grad_fn=<MseLossBackward0>)\n",
      "779 times loss: tensor(6.2599, grad_fn=<MseLossBackward0>)\n",
      "780 times loss: tensor(6.2520, grad_fn=<MseLossBackward0>)\n",
      "781 times loss: tensor(6.2441, grad_fn=<MseLossBackward0>)\n",
      "782 times loss: tensor(6.2362, grad_fn=<MseLossBackward0>)\n",
      "783 times loss: tensor(6.2283, grad_fn=<MseLossBackward0>)\n",
      "784 times loss: tensor(6.2205, grad_fn=<MseLossBackward0>)\n",
      "785 times loss: tensor(6.2127, grad_fn=<MseLossBackward0>)\n",
      "786 times loss: tensor(6.2049, grad_fn=<MseLossBackward0>)\n",
      "787 times loss: tensor(6.1971, grad_fn=<MseLossBackward0>)\n",
      "788 times loss: tensor(6.1893, grad_fn=<MseLossBackward0>)\n",
      "789 times loss: tensor(6.1816, grad_fn=<MseLossBackward0>)\n",
      "790 times loss: tensor(6.1738, grad_fn=<MseLossBackward0>)\n",
      "791 times loss: tensor(6.1661, grad_fn=<MseLossBackward0>)\n",
      "792 times loss: tensor(6.1584, grad_fn=<MseLossBackward0>)\n",
      "793 times loss: tensor(6.1507, grad_fn=<MseLossBackward0>)\n",
      "794 times loss: tensor(6.1431, grad_fn=<MseLossBackward0>)\n",
      "795 times loss: tensor(6.1354, grad_fn=<MseLossBackward0>)\n",
      "796 times loss: tensor(6.1278, grad_fn=<MseLossBackward0>)\n",
      "797 times loss: tensor(6.1202, grad_fn=<MseLossBackward0>)\n",
      "798 times loss: tensor(6.1126, grad_fn=<MseLossBackward0>)\n",
      "799 times loss: tensor(6.1050, grad_fn=<MseLossBackward0>)\n",
      "800 times loss: tensor(6.0975, grad_fn=<MseLossBackward0>)\n",
      "801 times loss: tensor(6.0899, grad_fn=<MseLossBackward0>)\n",
      "802 times loss: tensor(6.0824, grad_fn=<MseLossBackward0>)\n",
      "803 times loss: tensor(6.0749, grad_fn=<MseLossBackward0>)\n",
      "804 times loss: tensor(6.0674, grad_fn=<MseLossBackward0>)\n",
      "805 times loss: tensor(6.0599, grad_fn=<MseLossBackward0>)\n",
      "806 times loss: tensor(6.0524, grad_fn=<MseLossBackward0>)\n",
      "807 times loss: tensor(6.0450, grad_fn=<MseLossBackward0>)\n",
      "808 times loss: tensor(6.0376, grad_fn=<MseLossBackward0>)\n",
      "809 times loss: tensor(6.0301, grad_fn=<MseLossBackward0>)\n",
      "810 times loss: tensor(6.0228, grad_fn=<MseLossBackward0>)\n",
      "811 times loss: tensor(6.0154, grad_fn=<MseLossBackward0>)\n",
      "812 times loss: tensor(6.0080, grad_fn=<MseLossBackward0>)\n",
      "813 times loss: tensor(6.0007, grad_fn=<MseLossBackward0>)\n",
      "814 times loss: tensor(5.9933, grad_fn=<MseLossBackward0>)\n",
      "815 times loss: tensor(5.9860, grad_fn=<MseLossBackward0>)\n",
      "816 times loss: tensor(5.9787, grad_fn=<MseLossBackward0>)\n",
      "817 times loss: tensor(5.9714, grad_fn=<MseLossBackward0>)\n",
      "818 times loss: tensor(5.9642, grad_fn=<MseLossBackward0>)\n",
      "819 times loss: tensor(5.9569, grad_fn=<MseLossBackward0>)\n",
      "820 times loss: tensor(5.9497, grad_fn=<MseLossBackward0>)\n",
      "821 times loss: tensor(5.9425, grad_fn=<MseLossBackward0>)\n",
      "822 times loss: tensor(5.9353, grad_fn=<MseLossBackward0>)\n",
      "823 times loss: tensor(5.9281, grad_fn=<MseLossBackward0>)\n",
      "824 times loss: tensor(5.9209, grad_fn=<MseLossBackward0>)\n",
      "825 times loss: tensor(5.9137, grad_fn=<MseLossBackward0>)\n",
      "826 times loss: tensor(5.9066, grad_fn=<MseLossBackward0>)\n",
      "827 times loss: tensor(5.8995, grad_fn=<MseLossBackward0>)\n",
      "828 times loss: tensor(5.8924, grad_fn=<MseLossBackward0>)\n",
      "829 times loss: tensor(5.8853, grad_fn=<MseLossBackward0>)\n",
      "830 times loss: tensor(5.8782, grad_fn=<MseLossBackward0>)\n",
      "831 times loss: tensor(5.8712, grad_fn=<MseLossBackward0>)\n",
      "832 times loss: tensor(5.8641, grad_fn=<MseLossBackward0>)\n",
      "833 times loss: tensor(5.8571, grad_fn=<MseLossBackward0>)\n",
      "834 times loss: tensor(5.8501, grad_fn=<MseLossBackward0>)\n",
      "835 times loss: tensor(5.8431, grad_fn=<MseLossBackward0>)\n",
      "836 times loss: tensor(5.8361, grad_fn=<MseLossBackward0>)\n",
      "837 times loss: tensor(5.8291, grad_fn=<MseLossBackward0>)\n",
      "838 times loss: tensor(5.8222, grad_fn=<MseLossBackward0>)\n",
      "839 times loss: tensor(5.8153, grad_fn=<MseLossBackward0>)\n",
      "840 times loss: tensor(5.8084, grad_fn=<MseLossBackward0>)\n",
      "841 times loss: tensor(5.8015, grad_fn=<MseLossBackward0>)\n",
      "842 times loss: tensor(5.7946, grad_fn=<MseLossBackward0>)\n",
      "843 times loss: tensor(5.7877, grad_fn=<MseLossBackward0>)\n",
      "844 times loss: tensor(5.7808, grad_fn=<MseLossBackward0>)\n",
      "845 times loss: tensor(5.7740, grad_fn=<MseLossBackward0>)\n",
      "846 times loss: tensor(5.7672, grad_fn=<MseLossBackward0>)\n",
      "847 times loss: tensor(5.7604, grad_fn=<MseLossBackward0>)\n",
      "848 times loss: tensor(5.7536, grad_fn=<MseLossBackward0>)\n",
      "849 times loss: tensor(5.7468, grad_fn=<MseLossBackward0>)\n",
      "850 times loss: tensor(5.7400, grad_fn=<MseLossBackward0>)\n",
      "851 times loss: tensor(5.7333, grad_fn=<MseLossBackward0>)\n",
      "852 times loss: tensor(5.7265, grad_fn=<MseLossBackward0>)\n",
      "853 times loss: tensor(5.7198, grad_fn=<MseLossBackward0>)\n",
      "854 times loss: tensor(5.7131, grad_fn=<MseLossBackward0>)\n",
      "855 times loss: tensor(5.7064, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856 times loss: tensor(5.6997, grad_fn=<MseLossBackward0>)\n",
      "857 times loss: tensor(5.6931, grad_fn=<MseLossBackward0>)\n",
      "858 times loss: tensor(5.6864, grad_fn=<MseLossBackward0>)\n",
      "859 times loss: tensor(5.6798, grad_fn=<MseLossBackward0>)\n",
      "860 times loss: tensor(5.6732, grad_fn=<MseLossBackward0>)\n",
      "861 times loss: tensor(5.6666, grad_fn=<MseLossBackward0>)\n",
      "862 times loss: tensor(5.6600, grad_fn=<MseLossBackward0>)\n",
      "863 times loss: tensor(5.6534, grad_fn=<MseLossBackward0>)\n",
      "864 times loss: tensor(5.6468, grad_fn=<MseLossBackward0>)\n",
      "865 times loss: tensor(5.6403, grad_fn=<MseLossBackward0>)\n",
      "866 times loss: tensor(5.6338, grad_fn=<MseLossBackward0>)\n",
      "867 times loss: tensor(5.6272, grad_fn=<MseLossBackward0>)\n",
      "868 times loss: tensor(5.6207, grad_fn=<MseLossBackward0>)\n",
      "869 times loss: tensor(5.6142, grad_fn=<MseLossBackward0>)\n",
      "870 times loss: tensor(5.6077, grad_fn=<MseLossBackward0>)\n",
      "871 times loss: tensor(5.6013, grad_fn=<MseLossBackward0>)\n",
      "872 times loss: tensor(5.5948, grad_fn=<MseLossBackward0>)\n",
      "873 times loss: tensor(5.5884, grad_fn=<MseLossBackward0>)\n",
      "874 times loss: tensor(5.5820, grad_fn=<MseLossBackward0>)\n",
      "875 times loss: tensor(5.5755, grad_fn=<MseLossBackward0>)\n",
      "876 times loss: tensor(5.5692, grad_fn=<MseLossBackward0>)\n",
      "877 times loss: tensor(5.5628, grad_fn=<MseLossBackward0>)\n",
      "878 times loss: tensor(5.5564, grad_fn=<MseLossBackward0>)\n",
      "879 times loss: tensor(5.5500, grad_fn=<MseLossBackward0>)\n",
      "880 times loss: tensor(5.5437, grad_fn=<MseLossBackward0>)\n",
      "881 times loss: tensor(5.5374, grad_fn=<MseLossBackward0>)\n",
      "882 times loss: tensor(5.5311, grad_fn=<MseLossBackward0>)\n",
      "883 times loss: tensor(5.5248, grad_fn=<MseLossBackward0>)\n",
      "884 times loss: tensor(5.5185, grad_fn=<MseLossBackward0>)\n",
      "885 times loss: tensor(5.5122, grad_fn=<MseLossBackward0>)\n",
      "886 times loss: tensor(5.5059, grad_fn=<MseLossBackward0>)\n",
      "887 times loss: tensor(5.4997, grad_fn=<MseLossBackward0>)\n",
      "888 times loss: tensor(5.4934, grad_fn=<MseLossBackward0>)\n",
      "889 times loss: tensor(5.4872, grad_fn=<MseLossBackward0>)\n",
      "890 times loss: tensor(5.4810, grad_fn=<MseLossBackward0>)\n",
      "891 times loss: tensor(5.4748, grad_fn=<MseLossBackward0>)\n",
      "892 times loss: tensor(5.4686, grad_fn=<MseLossBackward0>)\n",
      "893 times loss: tensor(5.4625, grad_fn=<MseLossBackward0>)\n",
      "894 times loss: tensor(5.4563, grad_fn=<MseLossBackward0>)\n",
      "895 times loss: tensor(5.4502, grad_fn=<MseLossBackward0>)\n",
      "896 times loss: tensor(5.4440, grad_fn=<MseLossBackward0>)\n",
      "897 times loss: tensor(5.4379, grad_fn=<MseLossBackward0>)\n",
      "898 times loss: tensor(5.4318, grad_fn=<MseLossBackward0>)\n",
      "899 times loss: tensor(5.4257, grad_fn=<MseLossBackward0>)\n",
      "900 times loss: tensor(5.4196, grad_fn=<MseLossBackward0>)\n",
      "901 times loss: tensor(5.4135, grad_fn=<MseLossBackward0>)\n",
      "902 times loss: tensor(5.4075, grad_fn=<MseLossBackward0>)\n",
      "903 times loss: tensor(5.4014, grad_fn=<MseLossBackward0>)\n",
      "904 times loss: tensor(5.3954, grad_fn=<MseLossBackward0>)\n",
      "905 times loss: tensor(5.3894, grad_fn=<MseLossBackward0>)\n",
      "906 times loss: tensor(5.3834, grad_fn=<MseLossBackward0>)\n",
      "907 times loss: tensor(5.3774, grad_fn=<MseLossBackward0>)\n",
      "908 times loss: tensor(5.3714, grad_fn=<MseLossBackward0>)\n",
      "909 times loss: tensor(5.3654, grad_fn=<MseLossBackward0>)\n",
      "910 times loss: tensor(5.3595, grad_fn=<MseLossBackward0>)\n",
      "911 times loss: tensor(5.3535, grad_fn=<MseLossBackward0>)\n",
      "912 times loss: tensor(5.3476, grad_fn=<MseLossBackward0>)\n",
      "913 times loss: tensor(5.3417, grad_fn=<MseLossBackward0>)\n",
      "914 times loss: tensor(5.3357, grad_fn=<MseLossBackward0>)\n",
      "915 times loss: tensor(5.3298, grad_fn=<MseLossBackward0>)\n",
      "916 times loss: tensor(5.3239, grad_fn=<MseLossBackward0>)\n",
      "917 times loss: tensor(5.3181, grad_fn=<MseLossBackward0>)\n",
      "918 times loss: tensor(5.3122, grad_fn=<MseLossBackward0>)\n",
      "919 times loss: tensor(5.3063, grad_fn=<MseLossBackward0>)\n",
      "920 times loss: tensor(5.3005, grad_fn=<MseLossBackward0>)\n",
      "921 times loss: tensor(5.2947, grad_fn=<MseLossBackward0>)\n",
      "922 times loss: tensor(5.2888, grad_fn=<MseLossBackward0>)\n",
      "923 times loss: tensor(5.2830, grad_fn=<MseLossBackward0>)\n",
      "924 times loss: tensor(5.2772, grad_fn=<MseLossBackward0>)\n",
      "925 times loss: tensor(5.2714, grad_fn=<MseLossBackward0>)\n",
      "926 times loss: tensor(5.2657, grad_fn=<MseLossBackward0>)\n",
      "927 times loss: tensor(5.2599, grad_fn=<MseLossBackward0>)\n",
      "928 times loss: tensor(5.2542, grad_fn=<MseLossBackward0>)\n",
      "929 times loss: tensor(5.2484, grad_fn=<MseLossBackward0>)\n",
      "930 times loss: tensor(5.2427, grad_fn=<MseLossBackward0>)\n",
      "931 times loss: tensor(5.2370, grad_fn=<MseLossBackward0>)\n",
      "932 times loss: tensor(5.2313, grad_fn=<MseLossBackward0>)\n",
      "933 times loss: tensor(5.2256, grad_fn=<MseLossBackward0>)\n",
      "934 times loss: tensor(5.2199, grad_fn=<MseLossBackward0>)\n",
      "935 times loss: tensor(5.2142, grad_fn=<MseLossBackward0>)\n",
      "936 times loss: tensor(5.2086, grad_fn=<MseLossBackward0>)\n",
      "937 times loss: tensor(5.2029, grad_fn=<MseLossBackward0>)\n",
      "938 times loss: tensor(5.1973, grad_fn=<MseLossBackward0>)\n",
      "939 times loss: tensor(5.1916, grad_fn=<MseLossBackward0>)\n",
      "940 times loss: tensor(5.1860, grad_fn=<MseLossBackward0>)\n",
      "941 times loss: tensor(5.1804, grad_fn=<MseLossBackward0>)\n",
      "942 times loss: tensor(5.1748, grad_fn=<MseLossBackward0>)\n",
      "943 times loss: tensor(5.1692, grad_fn=<MseLossBackward0>)\n",
      "944 times loss: tensor(5.1636, grad_fn=<MseLossBackward0>)\n",
      "945 times loss: tensor(5.1581, grad_fn=<MseLossBackward0>)\n",
      "946 times loss: tensor(5.1525, grad_fn=<MseLossBackward0>)\n",
      "947 times loss: tensor(5.1470, grad_fn=<MseLossBackward0>)\n",
      "948 times loss: tensor(5.1414, grad_fn=<MseLossBackward0>)\n",
      "949 times loss: tensor(5.1359, grad_fn=<MseLossBackward0>)\n",
      "950 times loss: tensor(5.1304, grad_fn=<MseLossBackward0>)\n",
      "951 times loss: tensor(5.1249, grad_fn=<MseLossBackward0>)\n",
      "952 times loss: tensor(5.1194, grad_fn=<MseLossBackward0>)\n",
      "953 times loss: tensor(5.1139, grad_fn=<MseLossBackward0>)\n",
      "954 times loss: tensor(5.1085, grad_fn=<MseLossBackward0>)\n",
      "955 times loss: tensor(5.1030, grad_fn=<MseLossBackward0>)\n",
      "956 times loss: tensor(5.0976, grad_fn=<MseLossBackward0>)\n",
      "957 times loss: tensor(5.0921, grad_fn=<MseLossBackward0>)\n",
      "958 times loss: tensor(5.0867, grad_fn=<MseLossBackward0>)\n",
      "959 times loss: tensor(5.0813, grad_fn=<MseLossBackward0>)\n",
      "960 times loss: tensor(5.0759, grad_fn=<MseLossBackward0>)\n",
      "961 times loss: tensor(5.0705, grad_fn=<MseLossBackward0>)\n",
      "962 times loss: tensor(5.0651, grad_fn=<MseLossBackward0>)\n",
      "963 times loss: tensor(5.0597, grad_fn=<MseLossBackward0>)\n",
      "964 times loss: tensor(5.0543, grad_fn=<MseLossBackward0>)\n",
      "965 times loss: tensor(5.0490, grad_fn=<MseLossBackward0>)\n",
      "966 times loss: tensor(5.0436, grad_fn=<MseLossBackward0>)\n",
      "967 times loss: tensor(5.0383, grad_fn=<MseLossBackward0>)\n",
      "968 times loss: tensor(5.0330, grad_fn=<MseLossBackward0>)\n",
      "969 times loss: tensor(5.0277, grad_fn=<MseLossBackward0>)\n",
      "970 times loss: tensor(5.0224, grad_fn=<MseLossBackward0>)\n",
      "971 times loss: tensor(5.0171, grad_fn=<MseLossBackward0>)\n",
      "972 times loss: tensor(5.0118, grad_fn=<MseLossBackward0>)\n",
      "973 times loss: tensor(5.0065, grad_fn=<MseLossBackward0>)\n",
      "974 times loss: tensor(5.0013, grad_fn=<MseLossBackward0>)\n",
      "975 times loss: tensor(4.9960, grad_fn=<MseLossBackward0>)\n",
      "976 times loss: tensor(4.9908, grad_fn=<MseLossBackward0>)\n",
      "977 times loss: tensor(4.9855, grad_fn=<MseLossBackward0>)\n",
      "978 times loss: tensor(4.9803, grad_fn=<MseLossBackward0>)\n",
      "979 times loss: tensor(4.9751, grad_fn=<MseLossBackward0>)\n",
      "980 times loss: tensor(4.9699, grad_fn=<MseLossBackward0>)\n",
      "981 times loss: tensor(4.9647, grad_fn=<MseLossBackward0>)\n",
      "982 times loss: tensor(4.9595, grad_fn=<MseLossBackward0>)\n",
      "983 times loss: tensor(4.9544, grad_fn=<MseLossBackward0>)\n",
      "984 times loss: tensor(4.9492, grad_fn=<MseLossBackward0>)\n",
      "985 times loss: tensor(4.9440, grad_fn=<MseLossBackward0>)\n",
      "986 times loss: tensor(4.9389, grad_fn=<MseLossBackward0>)\n",
      "987 times loss: tensor(4.9338, grad_fn=<MseLossBackward0>)\n",
      "988 times loss: tensor(4.9286, grad_fn=<MseLossBackward0>)\n",
      "989 times loss: tensor(4.9235, grad_fn=<MseLossBackward0>)\n",
      "990 times loss: tensor(4.9184, grad_fn=<MseLossBackward0>)\n",
      "991 times loss: tensor(4.9133, grad_fn=<MseLossBackward0>)\n",
      "992 times loss: tensor(4.9082, grad_fn=<MseLossBackward0>)\n",
      "993 times loss: tensor(4.9032, grad_fn=<MseLossBackward0>)\n",
      "994 times loss: tensor(4.8981, grad_fn=<MseLossBackward0>)\n",
      "995 times loss: tensor(4.8930, grad_fn=<MseLossBackward0>)\n",
      "996 times loss: tensor(4.8880, grad_fn=<MseLossBackward0>)\n",
      "997 times loss: tensor(4.8830, grad_fn=<MseLossBackward0>)\n",
      "998 times loss: tensor(4.8779, grad_fn=<MseLossBackward0>)\n",
      "999 times loss: tensor(4.8729, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "import torch.optim as optim\n",
    "net = ToyNet()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.000001)\n",
    "epochs = 1000\n",
    "lloss = []\n",
    "for epoch in range(epochs):\n",
    "    #  计算输出\n",
    "    output = net(x_train)\n",
    "    #  计算损失值\n",
    "    loss = criterion(output, y_train)\n",
    "    #  清零梯度缓存\n",
    "    optimizer.zero_grad()\n",
    "    # 计算梯度\n",
    "    loss.backward()\n",
    "    #  更新参数\n",
    "    optimizer.step()\n",
    "    lloss.append(loss)\n",
    "    print(epoch, 'times loss:', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\nNzMrEYe+mVmJ/H+WZZNiouAWfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(epochs),lloss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 107., -885.],\n",
       "        [ 217., -242.],\n",
       "        [ 599., -169.],\n",
       "        ...,\n",
       "        [-358., -130.],\n",
       "        [-474.,  851.],\n",
       "        [-449.,  742.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygmt",
   "language": "python",
   "name": "pygmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
