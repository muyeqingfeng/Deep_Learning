{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as Fun\n",
    "\"\"\"\n",
    "1.定义一个类，这个类是你的模型，你定义的这个类的父类是（torch.nn.Module）,\n",
    "至少，最好必须要有两个函数，构造函数_init_ 用来初始化你的对象和forward用来做前馈运算。\n",
    "Module这个类已经默认自动帮你算反向传播过程了。你这个模型用到的是类是nn.Linear,这个类做的运算是y=Ax+b. bias就是b\n",
    "2.代价函数是：mse(torch.nn.MSELoss)\n",
    "3.优化器选择（optimizer）,随机梯度下降算法SGD（torch.optim.SGD）\n",
    "4.搭配上你之前的数据生成器来完成模型的训练，利用之前x,y\n",
    "5.总结来说就是：第一步算前馈过程的预测值；第二步算损失；第三步算backward，在算backward之间梯度清零；第四步，更新。\n",
    "\"\"\"\n",
    "\n",
    "class linear(nn.Module):\n",
    "    \n",
    "    # 定义网络层信息\n",
    "    def __init__(self, input_dim):\n",
    "        super(linear, self).__init__()\n",
    "        self.line1 = nn.Linear(input_dim, 16)\n",
    "        self.line2 = nn.Linear(16, 160)\n",
    "        self.line3 = nn.Linear(160, 1)\n",
    "    \n",
    "    # 构建前向传播计算过程\n",
    "    def forward(self, x):\n",
    "        y = Fun.relu(self.line1(x))\n",
    "        y = Fun.relu(self.line2(y))\n",
    "        y = self.line3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "\tdef __init__(self, x, y):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.x[index], self.y[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)\n",
    "\n",
    "np.random.seed(123)\n",
    "# x_data = torch.randn(100, 1)\n",
    "# y_data = 2 * x_data + 15 + torch.randn(100, 1)\n",
    "x_data = torch.Tensor([[i] for i in np.random.randint(1,100,200)])\n",
    "y_data = 2 * x_data + torch.Tensor([[i] for i in np.random.randint(-20,20,200)])\n",
    "tensor_dataset = TensorDataset(x_data, y_data)\n",
    "\n",
    "tensor_dataloader = DataLoader(tensor_dataset,   # 封装的对象\n",
    "                               batch_size=5,     # 输出的batch size\n",
    "                               shuffle=False,     # 随机输出\n",
    "                               num_workers=0)    # 只有1个进程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 time loss: tensor(5955.8550, grad_fn=<MseLossBackward0>)\n",
      "epoch: 1 time loss: tensor(4823.4961, grad_fn=<MseLossBackward0>)\n",
      "epoch: 2 time loss: tensor(3849.8608, grad_fn=<MseLossBackward0>)\n",
      "epoch: 3 time loss: tensor(3011.3545, grad_fn=<MseLossBackward0>)\n",
      "epoch: 4 time loss: tensor(2300.9292, grad_fn=<MseLossBackward0>)\n",
      "epoch: 5 time loss: tensor(1716.1321, grad_fn=<MseLossBackward0>)\n",
      "epoch: 6 time loss: tensor(1250.7815, grad_fn=<MseLossBackward0>)\n",
      "epoch: 7 time loss: tensor(894.9930, grad_fn=<MseLossBackward0>)\n",
      "epoch: 8 time loss: tensor(633.9927, grad_fn=<MseLossBackward0>)\n",
      "epoch: 9 time loss: tensor(450.2228, grad_fn=<MseLossBackward0>)\n",
      "epoch: 10 time loss: tensor(325.8170, grad_fn=<MseLossBackward0>)\n",
      "epoch: 11 time loss: tensor(244.6253, grad_fn=<MseLossBackward0>)\n",
      "epoch: 12 time loss: tensor(193.3962, grad_fn=<MseLossBackward0>)\n",
      "epoch: 13 time loss: tensor(162.0804, grad_fn=<MseLossBackward0>)\n",
      "epoch: 14 time loss: tensor(143.5190, grad_fn=<MseLossBackward0>)\n",
      "epoch: 15 time loss: tensor(132.8658, grad_fn=<MseLossBackward0>)\n",
      "epoch: 16 time loss: tensor(126.9741, grad_fn=<MseLossBackward0>)\n",
      "epoch: 17 time loss: tensor(123.8699, grad_fn=<MseLossBackward0>)\n",
      "epoch: 18 time loss: tensor(122.3502, grad_fn=<MseLossBackward0>)\n",
      "epoch: 19 time loss: tensor(121.7012, grad_fn=<MseLossBackward0>)\n",
      "epoch: 20 time loss: tensor(121.5095, grad_fn=<MseLossBackward0>)\n",
      "epoch: 21 time loss: tensor(121.5419, grad_fn=<MseLossBackward0>)\n",
      "epoch: 22 time loss: tensor(121.6711, grad_fn=<MseLossBackward0>)\n",
      "epoch: 23 time loss: tensor(121.8303, grad_fn=<MseLossBackward0>)\n",
      "epoch: 24 time loss: tensor(121.9868, grad_fn=<MseLossBackward0>)\n",
      "epoch: 25 time loss: tensor(122.1263, grad_fn=<MseLossBackward0>)\n",
      "epoch: 26 time loss: tensor(122.2442, grad_fn=<MseLossBackward0>)\n",
      "epoch: 27 time loss: tensor(122.3407, grad_fn=<MseLossBackward0>)\n",
      "epoch: 28 time loss: tensor(122.4180, grad_fn=<MseLossBackward0>)\n",
      "epoch: 29 time loss: tensor(122.4789, grad_fn=<MseLossBackward0>)\n",
      "epoch: 30 time loss: tensor(122.5266, grad_fn=<MseLossBackward0>)\n",
      "epoch: 31 time loss: tensor(122.5636, grad_fn=<MseLossBackward0>)\n",
      "epoch: 32 time loss: tensor(122.5923, grad_fn=<MseLossBackward0>)\n",
      "epoch: 33 time loss: tensor(122.6143, grad_fn=<MseLossBackward0>)\n",
      "epoch: 34 time loss: tensor(122.6311, grad_fn=<MseLossBackward0>)\n",
      "epoch: 35 time loss: tensor(122.6441, grad_fn=<MseLossBackward0>)\n",
      "epoch: 36 time loss: tensor(122.6541, grad_fn=<MseLossBackward0>)\n",
      "epoch: 37 time loss: tensor(122.6618, grad_fn=<MseLossBackward0>)\n",
      "epoch: 38 time loss: tensor(122.6676, grad_fn=<MseLossBackward0>)\n",
      "epoch: 39 time loss: tensor(122.6722, grad_fn=<MseLossBackward0>)\n",
      "epoch: 40 time loss: tensor(122.6756, grad_fn=<MseLossBackward0>)\n",
      "epoch: 41 time loss: tensor(122.6781, grad_fn=<MseLossBackward0>)\n",
      "epoch: 42 time loss: tensor(122.6802, grad_fn=<MseLossBackward0>)\n",
      "epoch: 43 time loss: tensor(122.6818, grad_fn=<MseLossBackward0>)\n",
      "epoch: 44 time loss: tensor(122.6831, grad_fn=<MseLossBackward0>)\n",
      "epoch: 45 time loss: tensor(122.6842, grad_fn=<MseLossBackward0>)\n",
      "epoch: 46 time loss: tensor(122.6850, grad_fn=<MseLossBackward0>)\n",
      "epoch: 47 time loss: tensor(122.6858, grad_fn=<MseLossBackward0>)\n",
      "epoch: 48 time loss: tensor(122.6863, grad_fn=<MseLossBackward0>)\n",
      "epoch: 49 time loss: tensor(122.6869, grad_fn=<MseLossBackward0>)\n",
      "epoch: 50 time loss: tensor(122.6874, grad_fn=<MseLossBackward0>)\n",
      "epoch: 51 time loss: tensor(122.6878, grad_fn=<MseLossBackward0>)\n",
      "epoch: 52 time loss: tensor(122.6882, grad_fn=<MseLossBackward0>)\n",
      "epoch: 53 time loss: tensor(122.6886, grad_fn=<MseLossBackward0>)\n",
      "epoch: 54 time loss: tensor(122.6889, grad_fn=<MseLossBackward0>)\n",
      "epoch: 55 time loss: tensor(122.6891, grad_fn=<MseLossBackward0>)\n",
      "epoch: 56 time loss: tensor(122.6895, grad_fn=<MseLossBackward0>)\n",
      "epoch: 57 time loss: tensor(122.6898, grad_fn=<MseLossBackward0>)\n",
      "epoch: 58 time loss: tensor(122.6901, grad_fn=<MseLossBackward0>)\n",
      "epoch: 59 time loss: tensor(122.6905, grad_fn=<MseLossBackward0>)\n",
      "epoch: 60 time loss: tensor(122.6909, grad_fn=<MseLossBackward0>)\n",
      "epoch: 61 time loss: tensor(122.6911, grad_fn=<MseLossBackward0>)\n",
      "epoch: 62 time loss: tensor(122.6914, grad_fn=<MseLossBackward0>)\n",
      "epoch: 63 time loss: tensor(122.6918, grad_fn=<MseLossBackward0>)\n",
      "epoch: 64 time loss: tensor(122.6921, grad_fn=<MseLossBackward0>)\n",
      "epoch: 65 time loss: tensor(122.6925, grad_fn=<MseLossBackward0>)\n",
      "epoch: 66 time loss: tensor(122.6928, grad_fn=<MseLossBackward0>)\n",
      "epoch: 67 time loss: tensor(122.6931, grad_fn=<MseLossBackward0>)\n",
      "epoch: 68 time loss: tensor(122.6934, grad_fn=<MseLossBackward0>)\n",
      "epoch: 69 time loss: tensor(122.6937, grad_fn=<MseLossBackward0>)\n",
      "epoch: 70 time loss: tensor(122.6939, grad_fn=<MseLossBackward0>)\n",
      "epoch: 71 time loss: tensor(122.6941, grad_fn=<MseLossBackward0>)\n",
      "epoch: 72 time loss: tensor(122.6944, grad_fn=<MseLossBackward0>)\n",
      "epoch: 73 time loss: tensor(122.6947, grad_fn=<MseLossBackward0>)\n",
      "epoch: 74 time loss: tensor(122.6948, grad_fn=<MseLossBackward0>)\n",
      "epoch: 75 time loss: tensor(122.6944, grad_fn=<MseLossBackward0>)\n",
      "epoch: 76 time loss: tensor(122.6940, grad_fn=<MseLossBackward0>)\n",
      "epoch: 77 time loss: tensor(122.6939, grad_fn=<MseLossBackward0>)\n",
      "epoch: 78 time loss: tensor(122.6938, grad_fn=<MseLossBackward0>)\n",
      "epoch: 79 time loss: tensor(122.6938, grad_fn=<MseLossBackward0>)\n",
      "epoch: 80 time loss: tensor(122.6939, grad_fn=<MseLossBackward0>)\n",
      "epoch: 81 time loss: tensor(122.6939, grad_fn=<MseLossBackward0>)\n",
      "epoch: 82 time loss: tensor(122.6939, grad_fn=<MseLossBackward0>)\n",
      "epoch: 83 time loss: tensor(122.6940, grad_fn=<MseLossBackward0>)\n",
      "epoch: 84 time loss: tensor(122.6940, grad_fn=<MseLossBackward0>)\n",
      "epoch: 85 time loss: tensor(122.6941, grad_fn=<MseLossBackward0>)\n",
      "epoch: 86 time loss: tensor(122.6941, grad_fn=<MseLossBackward0>)\n",
      "epoch: 87 time loss: tensor(122.6941, grad_fn=<MseLossBackward0>)\n",
      "epoch: 88 time loss: tensor(122.6942, grad_fn=<MseLossBackward0>)\n",
      "epoch: 89 time loss: tensor(122.6943, grad_fn=<MseLossBackward0>)\n",
      "epoch: 90 time loss: tensor(122.6943, grad_fn=<MseLossBackward0>)\n",
      "epoch: 91 time loss: tensor(122.6943, grad_fn=<MseLossBackward0>)\n",
      "epoch: 92 time loss: tensor(122.6943, grad_fn=<MseLossBackward0>)\n",
      "epoch: 93 time loss: tensor(122.6944, grad_fn=<MseLossBackward0>)\n",
      "epoch: 94 time loss: tensor(122.6944, grad_fn=<MseLossBackward0>)\n",
      "epoch: 95 time loss: tensor(122.6946, grad_fn=<MseLossBackward0>)\n",
      "epoch: 96 time loss: tensor(122.6947, grad_fn=<MseLossBackward0>)\n",
      "epoch: 97 time loss: tensor(122.6947, grad_fn=<MseLossBackward0>)\n",
      "epoch: 98 time loss: tensor(122.6948, grad_fn=<MseLossBackward0>)\n",
      "epoch: 99 time loss: tensor(122.6949, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 实例化一个类\n",
    "net = linear(1) \n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0000001)\n",
    "\n",
    "# 定义损失函数\n",
    "cost = nn.MSELoss()\n",
    "\n",
    "# 开始训练\n",
    "epochs = 100\n",
    "lloss = []\n",
    "for epoch in range(epochs):\n",
    "    for (x_train, y_train) in tensor_dataloader:\n",
    "        # 计算输出\n",
    "        output = net(x_train)\n",
    "        # 计算损失值\n",
    "        loss = cost(output, y_train)\n",
    "        # 清零梯度缓存\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        \n",
    "    lloss.append(loss)\n",
    "    print('epoch:', epoch, 'time loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(lloss)), lloss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygmt",
   "language": "python",
   "name": "pygmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
